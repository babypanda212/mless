{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9040d5e5",
      "metadata": {
        "id": "9040d5e5"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maschu09/mless/blob/main/time_series_forecasting/4_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c615ad",
      "metadata": {
        "id": "c8c615ad"
      },
      "source": [
        "# Author: Ayesha Khan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HS4cFVs4ZpYX",
      "metadata": {
        "id": "HS4cFVs4ZpYX"
      },
      "source": [
        "# Table of Contents:\n",
        "\n",
        "  ## 1. Initial Setup\n",
        "  ## 2. Helper Functions\n",
        "  ## 3. Data Download\n",
        "  ## 4. Data Preprocessing\n",
        "  ## 5. Data Preparation for MLP + Granger Analysis\n",
        "  ## 6. Model Setup\n",
        "  ## 7. Training\n",
        "  ## 8. Evaluation\n",
        "  ## 9. Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49eb9d93",
      "metadata": {
        "id": "49eb9d93"
      },
      "source": [
        "# Initial setup, declarations and method definitions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b4eba776",
      "metadata": {
        "id": "b4eba776"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oHVmTJo-yF3z",
      "metadata": {
        "id": "oHVmTJo-yF3z"
      },
      "source": [
        "## Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fc809476",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc809476",
        "outputId": "82454cc8-4832-450d-fd75-0f0b27ea186b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Mount google drive when working in colab\n",
        "hasCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
        "if hasCOLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  BASEPATH = '/content/drive/MyDrive'\n",
        "else:\n",
        "  BASEPATH = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e51ceddc",
      "metadata": {
        "id": "e51ceddc"
      },
      "outputs": [],
      "source": [
        "# Global constants\n",
        "TIMESERIES_DATA_DIR = BASEPATH + \"/timeseries_data/\"\n",
        "# Storage of actual values\n",
        "TIMESERIES_CSV_DIR = os.path.join(TIMESERIES_DATA_DIR, \"toar_csv_timeseries\")\n",
        "# Storage of Timeseries IDs\n",
        "TIMESERIES_ID_FILE = os.path.join(TIMESERIES_DATA_DIR, \"timeseriesIDs.json\")\n",
        "# Min file size for sanity check\n",
        "MIN_FILE_SIZE_BYTES = 100\n",
        "# expected columns from download\n",
        "group_columns = ['station_code', 'latitude', 'longitude']\n",
        "\n",
        "# create directories if not existing\n",
        "os.makedirs(TIMESERIES_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(TIMESERIES_CSV_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9f8684a8",
      "metadata": {
        "id": "9f8684a8"
      },
      "outputs": [],
      "source": [
        "# MLP related imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# set length of past and future time series\n",
        "# to be used in the model\n",
        "# context_window is the number of past time steps\n",
        "# prediction_horizon is the number of future time steps\n",
        "context_window = 336\n",
        "prediction_horizon = 96"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rDOxC01RXVvy",
      "metadata": {
        "id": "rDOxC01RXVvy"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcwMplcGYFua",
      "metadata": {
        "id": "bcwMplcGYFua"
      },
      "source": [
        "1. load_existing_timeseries,\n",
        "2. save_timeseries_ids,\n",
        "3. fetch_timeseries_data,\n",
        "4. pivot handle,\n",
        "5. csv download,\n",
        "6. evaluate model\n",
        "7. results saver (from task 1)\n",
        "8. fill_six_nans\n",
        "9. log transform if skewed\n",
        "10. standard scaler\n",
        "11. create sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d783afb2",
      "metadata": {
        "id": "d783afb2"
      },
      "outputs": [],
      "source": [
        "def load_existing_timeseries_ids():\n",
        "    \"\"\"\n",
        "    Load existing timeseries IDs from a JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing stored timeseries metadata.\n",
        "    \"\"\"\n",
        "    return json.load(open(TIMESERIES_ID_FILE, 'r')) if os.path.exists(TIMESERIES_ID_FILE) else {}\n",
        "\n",
        "def save_timeseries_ids(timeseries_data):\n",
        "    \"\"\"\n",
        "    Save timeseries metadata to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        timeseries_data (dict): A dictionary containing timeseries metadata.\n",
        "    \"\"\"\n",
        "    json.dump(timeseries_data, open(TIMESERIES_ID_FILE, 'w'), indent=4)\n",
        "\n",
        "def fetch_timeseries_data(station_codes, existing_timeseries, variable_columns):\n",
        "    \"\"\"\n",
        "    Fetch timeseries metadata for given station codes, filtering by specified variables.\n",
        "\n",
        "    Args:\n",
        "        station_codes (list): List of station codes to fetch data for.\n",
        "        existing_timeseries (dict): Dictionary of previously fetched timeseries metadata.\n",
        "        variable_columns (list): List of variable names to retain.\n",
        "\n",
        "    Returns:\n",
        "        dict: Updated dictionary containing filtered timeseries metadata.\n",
        "    \"\"\"\n",
        "    base_url = \"http://toar-data.fz-juelich.de/api/v2/search/?codes=\"\n",
        "    unique_entries = existing_timeseries.copy()\n",
        "    processed_station_codes = {details['station_code'] for details in existing_timeseries.values()}\n",
        "\n",
        "    for code in station_codes:\n",
        "        if code in processed_station_codes:\n",
        "            print(f\"\\t\\tStation {code} is already processed, skipping.\")\n",
        "            continue\n",
        "\n",
        "        response = requests.get(base_url + code, timeout=1000)\n",
        "        if response.status_code == 200:\n",
        "            for entry in response.json():\n",
        "                if (variable_name := entry.get('variable', {}).get('name')) in variable_columns:\n",
        "                    timeseries_id = entry.get('id')\n",
        "                    if timeseries_id not in unique_entries:\n",
        "                        unique_entries[timeseries_id] = {\n",
        "                            'data_start_date': entry.get('data_start_date'),\n",
        "                            'data_end_date': entry.get('data_end_date'),\n",
        "                            'variable_name': variable_name,\n",
        "                            'station_code': code,\n",
        "                            'latitude': entry.get('station', {}).get('coordinates', {}).get('lat'),\n",
        "                            'longitude': entry.get('station', {}).get('coordinates', {}).get('lng'),\n",
        "                        }\n",
        "        else:\n",
        "            print(f\"\\t\\tFailed to fetch data for station {code}. Status code: {response.status_code}\")\n",
        "    return unique_entries\n",
        "\n",
        "def pivot_handle(dfs, metadata_columns, variable_columns):\n",
        "    \"\"\"\n",
        "    Pivot and structure the timeseries dataframe for sequential data analysis.\n",
        "\n",
        "    Args:\n",
        "        dfs (pd.DataFrame): Dataframe containing timeseries data.\n",
        "        metadata_columns (list): List of metadata column names.\n",
        "        variable_columns (list): List of variable names to include.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed dataframe with pivoted structure.\n",
        "    \"\"\"\n",
        "    pivot_df = dfs.pivot_table(index='datetime', columns='variable_name', values='value', aggfunc='mean')\n",
        "    pivot_df.reset_index(inplace=True)\n",
        "\n",
        "    print(f\"Station {dfs['station_code'].unique()} min time: {dfs['datetime'].min()}, max time: {dfs['datetime'].max()}, hours between: {(dfs['datetime'].max() - dfs['datetime'].min()) / pd.Timedelta(hours=1):.2f}\")\n",
        "    reference_index = pd.date_range(start=dfs['datetime'].min(), end=dfs['datetime'].max(), freq=\"h\", tz=\"UTC\")\n",
        "    reference_df = pd.DataFrame({'datetime': reference_index})\n",
        "\n",
        "    pivot_df_ = reference_df.merge(pivot_df, on='datetime', how='left')\n",
        "\n",
        "    for col in metadata_columns:\n",
        "        if dfs[col].notna().any():\n",
        "            value = dfs[col].dropna().iloc[0]\n",
        "            pivot_df_.insert(0, col, value)\n",
        "        else:\n",
        "            print(f\"Station {dfs['station_code'].unique()}: metadata {col} has no value\")\n",
        "\n",
        "    return pivot_df_\n",
        "\n",
        "def download_csv_data(timeseries_data, variable_columns):\n",
        "    \"\"\"\n",
        "    Download and process CSV data for each timeseries ID.\n",
        "\n",
        "    Args:\n",
        "        timeseries_data (dict): Dictionary containing timeseries metadata.\n",
        "        variable_columns (list): List of variable names to process.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Combined dataframe of all timeseries data.\n",
        "    \"\"\"\n",
        "    dataframes = []\n",
        "    metadata_columns = ['station_code', 'latitude', 'longitude']\n",
        "\n",
        "    for ts_id, details in timeseries_data.items():\n",
        "        csv_path = os.path.join(TIMESERIES_CSV_DIR, f\"{ts_id}.csv\")\n",
        "\n",
        "        if os.path.exists(csv_path) and os.path.getsize(csv_path) > MIN_FILE_SIZE_BYTES:\n",
        "            print(f\"\\tCSV already exists for timeseries ID {ts_id}, skipping download.\")\n",
        "        else:\n",
        "            print(f\"\\tDownloading data for timeseries ID {ts_id}\")\n",
        "            url = f\"http://toar-data.fz-juelich.de/api/v2/data/timeseries/{ts_id}?format=csv\"\n",
        "            try:\n",
        "                response = requests.get(url, stream=True, timeout=1000)\n",
        "                response.raise_for_status()\n",
        "                with open(csv_path, 'wb') as file:\n",
        "                    file.writelines(response.iter_content(chunk_size=8192))\n",
        "                print(f\"\\t\\tRaw data CSV of {ts_id} saved: {csv_path}\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"\\t\\tFailed to download data for timeseries ID {ts_id}. Error: {e}\")\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path, skiprows=lambda i: i < next(i for i, line in enumerate(open(csv_path)) if line.startswith('datetime')), low_memory=False)\n",
        "            df['datetime'] = pd.to_datetime(df['datetime'], format='mixed')\n",
        "            df[['variable_name', 'station_code', 'latitude', 'longitude']] = details['variable_name'], details['station_code'], details['latitude'], details['longitude']\n",
        "            print(f\"Dataframe for timeseries ID {ts_id} loaded successfully with shape {df.shape}\")\n",
        "            dataframes.append(pivot_handle(df, metadata_columns, variable_columns))\n",
        "        except (pd.errors.EmptyDataError, pd.errors.ParserError) as e:\n",
        "            print(f\"\\tError processing CSV for timeseries ID {ts_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.concat(dataframes, ignore_index=True).sort_values(by=['station_code', 'datetime']) if dataframes else pd.DataFrame()\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4adeb9ea",
      "metadata": {
        "id": "4adeb9ea"
      },
      "outputs": [],
      "source": [
        "# function to save forecast results in a structured format\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "def save_forecast(\n",
        "        station: str,\n",
        "        model: str,\n",
        "        context_vals,               # 1-D array-like, X test values\n",
        "        future_true,                # 1-D array-like, true future values\n",
        "        future_pred,                # 1-D array-like, predicted future values\n",
        "        episode_id,                 # int index, to identify the timeperiod\n",
        "        folder=f\"{BASEPATH}/results\"): # default folder for results\n",
        "    \"\"\"\n",
        "    One call per model-run. Stores just enough metadata to let the\n",
        "    plotting notebook know what itâ€™s looking at.\n",
        "    \"\"\"\n",
        "    import os, json, numpy as np\n",
        "    # make sure the base path exists\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    # structure the result\n",
        "    # note: context_vals, future_true, and future_pred should be 1-D arrays\n",
        "    result = {\n",
        "        \"station\":        station,              # <- allows filtering\n",
        "        \"episode_id\":     episode_id,           # <- allows filtering\n",
        "        \"model\":          model,                # <- allows choosing of model\n",
        "        \"context\":        np.asarray(context_vals).tolist(),    # converts to numpy array and then to list\n",
        "        \"future_true\":    np.asarray(future_true).tolist(),\n",
        "        \"future_pred\":    np.asarray(future_pred).tolist()\n",
        "    }\n",
        "\n",
        "    # create a filename based on model, station, and episode_id\n",
        "    fname = f\"{model}_{station}_{episode_id}.json\"\n",
        "\n",
        "    # save the result to a JSON file\n",
        "    with open(os.path.join(folder, fname), \"w\") as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "\n",
        "    # let user know station code and episode_id for plotting purpose\n",
        "    print(f\"Results saved as {fname}, episode_id is {episode_id} and station is {station}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3uQAB9LXON5j",
      "metadata": {
        "id": "3uQAB9LXON5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fill_six_nans(group):\n",
        "    \"\"\"\n",
        "    Fills up to six consecutive NaN values in a given pandas Series using linear interpolation\n",
        "    if the NaNs are surrounded by valid values. If the NaNs are at the start, they are replaced\n",
        "    with zeros, and if they are at the end, they are filled with the last known value.\n",
        "\n",
        "    Args:\n",
        "        group (pd.Series): The input Series with potential NaN values.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series where up to six consecutive NaNs are interpolated, and longer NaN\n",
        "        sequences are partially filled while preserving the original index.\n",
        "    \"\"\"\n",
        "    values = group.to_numpy()\n",
        "    i = 0\n",
        "    while i < len(values):\n",
        "        if np.isnan(values[i]):\n",
        "            start = i\n",
        "            while i < len(values) and np.isnan(values[i]):\n",
        "                i += 1\n",
        "            end = min(i, start + 6)  # Limit to filling only 6 NaNs\n",
        "\n",
        "            if start > 0 and i < len(values):  # NaNs in the middle\n",
        "                fill_values = np.linspace(values[start - 1], values[i], end - start + 2)[1:-1]\n",
        "            elif start == 0:  # NaNs at the start\n",
        "                fill_values = [0] * (end - start)\n",
        "            elif i >= len(values):  # NaNs at the end\n",
        "                fill_values = [values[start - 1]] * (end - start)\n",
        "            values[start:end] = fill_values\n",
        "        else:\n",
        "            i += 1\n",
        "    return pd.Series(values, index=group.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "B5RHWNiheMYD",
      "metadata": {
        "id": "B5RHWNiheMYD"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import skew\n",
        "\n",
        "def log_transform_if_skewed(df, columns, threshold=1.0):\n",
        "    \"\"\"\n",
        "    Log-transform the specified columns of a DataFrame based on their skewdness.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        columns (list): List of column names that need to be checked for skewdness.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with normalized columns.\n",
        "    \"\"\"\n",
        "    df_transformed = df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        # s = df[col].dropna()\n",
        "        s = df[col]\n",
        "        current_skewness = skew(s)\n",
        "\n",
        "        print(f\"[{col}] Skewness: {current_skewness:.2f}\")\n",
        "\n",
        "        if abs(current_skewness) > threshold:\n",
        "            # To avoid log(0) or log(negative values).\n",
        "            if (s <= 0).any():\n",
        "                shift = abs(s.min()) + 1e-6\n",
        "                print(f\"Applying log(x + {shift:.6f}) to {col}\")\n",
        "                df_transformed[col] = np.log(df[col] + shift)\n",
        "            else:\n",
        "                print(f\"Applying log(x) to {col}\")\n",
        "                df_transformed[col] = np.log(df[col])\n",
        "        else:\n",
        "            print(f\"No transformation applied to {col}.\")\n",
        "\n",
        "    return df_transformed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standard_scaler(df, columns):\n",
        "    \"\"\"\n",
        "    Standardize the specified columns of a DataFrame by subtracting the mean\n",
        "    and dividing by the standard deviation (Z-score normalization).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        columns (list): List of column names to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with normalized columns.\n",
        "    \"\"\"\n",
        "    df_scaled = df.copy()\n",
        "    for col in columns:\n",
        "        mean = df_scaled[col].mean()\n",
        "        std = df_scaled[col].std()\n",
        "        df_scaled[col] = (df_scaled[col] - mean) / std\n",
        "    return df_scaled"
      ],
      "metadata": {
        "id": "VbQtenve606u"
      },
      "id": "VbQtenve606u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "UJKeh1FneL-x",
      "metadata": {
        "id": "UJKeh1FneL-x"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "TynS8Wxnl8vY",
      "metadata": {
        "id": "TynS8Wxnl8vY"
      },
      "outputs": [],
      "source": [
        "# Function to create continous time-series data with past 24 hours as input and next 6 hours as output\n",
        "def create_sequences(data, variable_column, n_past=336, n_future=96):\n",
        "    df = data.copy()\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "    df = df.sort_values(['station_code', 'datetime']).reset_index(drop=True)\n",
        "\n",
        "    # 1) Encode station_code as a small integer (categorical codes) to save memory\n",
        "    df['sc_code'] = df['station_code'].astype('category')\n",
        "\n",
        "    # 2) Boolean mask: True if exactly 1 hour after the previous row (per station)\n",
        "    is1h = (\n",
        "        df.groupby('station_code')['datetime']\n",
        "          .diff()\n",
        "          .eq(pd.Timedelta(hours=1))\n",
        "    )\n",
        "\n",
        "    # 3) Compute how many consecutive â€œTrueâ€ over (n_past + n_future - 1) rows\n",
        "    gap_window = n_past + n_future - 1\n",
        "    run = (\n",
        "        is1h.groupby(df['station_code'])\n",
        "            .rolling(window=gap_window, min_periods=gap_window)\n",
        "            .sum()\n",
        "            .reset_index(level=0, drop=True)\n",
        "    )\n",
        "\n",
        "    # 4) 'ends' are indices where run == gap_window (end of a full-length continuous block)\n",
        "    ends = run[run == gap_window].index\n",
        "\n",
        "    # 5) Grab columnâ€arrays for encoded station code and the variable\n",
        "    sc_arr  = df['sc_code'].values\n",
        "    vals    = df[variable_column].values\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    for end_idx in ends:\n",
        "        start_idx = end_idx - gap_window\n",
        "        if start_idx >= 0 and sc_arr[start_idx] == sc_arr[end_idx]:\n",
        "            station_block = sc_arr[start_idx : end_idx + 1]\n",
        "            var_block     = vals[start_idx : end_idx + 1]\n",
        "\n",
        "            block = np.column_stack((station_block, var_block))\n",
        "\n",
        "            X_list.append(block[:n_past, :])\n",
        "            y_list.append(block[n_past:, :])\n",
        "\n",
        "    if not X_list:\n",
        "        # No valid windows\n",
        "        return np.empty((0, n_past, 2)), np.empty((0, n_future, 2))\n",
        "\n",
        "    X = np.stack(X_list)  # shape = (num_windows, n_past, 2)\n",
        "    y = np.stack(y_list)  # shape = (num_windows, n_future, 2)\n",
        "    return X, y\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8_I6FbZMYmqO",
      "metadata": {
        "id": "8_I6FbZMYmqO"
      },
      "source": [
        "# Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fNDdjHgyXvMz",
      "metadata": {
        "id": "fNDdjHgyXvMz"
      },
      "outputs": [],
      "source": [
        "# select station for download\n",
        "station_codes = [\"DENW094\"]\n",
        "# select variables for download\n",
        "variable_columns = [\"temp\", \"o3\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xAc3wxr3Y982",
      "metadata": {
        "id": "xAc3wxr3Y982"
      },
      "source": [
        "## Download via REST API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bIELzP2mY9gz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIELzP2mY9gz",
        "outputId": "8ac03199-2951-42d4-8cd0-f68b2d6cf4cf"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\t\tStation DENW094 is already processed, skipping.\n",
            "\t Number of time series meta data fetched : 3\n",
            "\tCSV already exists for timeseries ID 76, skipping download.\n"
          ]
        }
      ],
      "source": [
        "# Load existing timeseries IDs from json to skip calls to TOAR\n",
        "existing_timeseries = load_existing_timeseries_ids()\n",
        "\n",
        "# download timeseries IDs required\n",
        "timeseries_data = fetch_timeseries_data(station_codes, existing_timeseries,variable_columns)\n",
        "print(f\"\\t Number of time series meta data fetched : {len(timeseries_data)}\")\n",
        "\n",
        "# save existing timeseries IDs as json to reduce calls to TOAR in future\n",
        "save_timeseries_ids(timeseries_data)\n",
        "\n",
        "# download actual data and make into dataframe\n",
        "dataframes = download_csv_data(timeseries_data,variable_columns)\n",
        "print(f\"\\t Total dataFrames processed : {len(dataframes)} and shape of first dataframe {dataframes.shape}.\")\n",
        "\n",
        "dataframes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F6MRdgvscWTS",
      "metadata": {
        "id": "F6MRdgvscWTS"
      },
      "outputs": [],
      "source": [
        "# save data in case session timeout\n",
        "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VPF_l5McczAy",
      "metadata": {
        "id": "VPF_l5McczAy"
      },
      "source": [
        "Code in case of timeout to load data again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dB3k8jBCc0O8",
      "metadata": {
        "id": "dB3k8jBCc0O8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# save data path variable\n",
        "data_path = os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\")\n",
        "# read in raw data\n",
        "dataframes = pd.read_csv(data_path,parse_dates=[\"datetime\"])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GgDSGws-Zm-e",
      "metadata": {
        "id": "GgDSGws-Zm-e"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QTh-HCFhaWq9",
      "metadata": {
        "id": "QTh-HCFhaWq9"
      },
      "source": [
        "## Dealing with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mA_p0yV0aZIO",
      "metadata": {
        "id": "mA_p0yV0aZIO"
      },
      "outputs": [],
      "source": [
        "# groups values by variable and fills in six NaN values\n",
        "dataframes[variable_columns] = dataframes.groupby(group_columns)[variable_columns].transform(fill_six_nans)\n",
        "# checks number of na values\n",
        "dataframes.isna().sum()\n",
        "# drop remaining na values\n",
        "dataframes = dataframes.dropna()\n",
        "# check again\n",
        "dataframes.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qd4YM9V0Smu8",
      "metadata": {
        "id": "qd4YM9V0Smu8"
      },
      "source": [
        "## Statistical Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61S7XC7zNU1y",
      "metadata": {
        "id": "61S7XC7zNU1y"
      },
      "outputs": [],
      "source": [
        "stats = ['min', 'max', 'mean', 'sum', 'std', 'var', 'median','prod','nunique',\n",
        "    ('5th_percentile', lambda x: x.quantile(0.05)),\n",
        "    ('10th_percentile', lambda x: x.quantile(0.10)),\n",
        "    ('25th_percentile', lambda x: x.quantile(0.25)),\n",
        "    ('50th_percentile', lambda x: x.quantile(0.50)), #(median)\n",
        "    ('75th_percentile', lambda x: x.quantile(0.75))]\n",
        "agg_dict = {col: stats for col in variable_columns}\n",
        "grouped = dataframes.groupby('station_code').agg(agg_dict)\n",
        "display(grouped)\n",
        "\n",
        "for agg_func in ['min', 'max', 'mean', 'std']:\n",
        "    display(agg_func)\n",
        "    agg_view = grouped.xs(agg_func, axis=1, level=1)\n",
        "    display(agg_view)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vO5hGVrwehyz",
      "metadata": {
        "id": "vO5hGVrwehyz"
      },
      "source": [
        "# Data Preparation for MLP + Granger Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bUkal4KWgRc9",
      "metadata": {
        "id": "bUkal4KWgRc9"
      },
      "source": [
        "## Improving ditribution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N7BFnIL-eQZ6",
      "metadata": {
        "id": "N7BFnIL-eQZ6"
      },
      "source": [
        "### Log Scaling after checking for skewdness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cxGV6_LeXLz",
      "metadata": {
        "id": "6cxGV6_LeXLz"
      },
      "outputs": [],
      "source": [
        "# log transofmr data if distribution is not gaussian\n",
        "dataframe_= log_transform_if_skewed(dataframes, variable_columns, threshold=1.0)\n",
        "dataframe_.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nog3ce9heoBq",
      "metadata": {
        "id": "nog3ce9heoBq"
      },
      "source": [
        "### Normalise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "emmHuMtpfC2T",
      "metadata": {
        "id": "emmHuMtpfC2T"
      },
      "outputs": [],
      "source": [
        "# z-normalisation of data\n",
        "dataframes = standard_scaler(dataframe_, variable_columns)\n",
        "# show dataframe\n",
        "dataframes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-cc-ixdbc_QR",
      "metadata": {
        "id": "-cc-ixdbc_QR"
      },
      "source": [
        "##### Save the normalized dataframe for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oi3PPzzJZ5Vf",
      "metadata": {
        "id": "Oi3PPzzJZ5Vf"
      },
      "outputs": [],
      "source": [
        "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"normalized_data.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6c8376",
      "metadata": {
        "id": "7d6c8376"
      },
      "source": [
        "## Granger Analysis\n",
        "\n",
        "ðŸ˜ˆ **Task 3:** Run a Granger test between `temp` and `o3`. Is there any directional causality?\n",
        "\n",
        "ðŸ˜ˆ **Question 3:** Why is Granger causality not the same as actual causality?\n",
        "Granger causality only checks if it is possible to predict the relationship between two series; it does not establish the actual causality. So it could be only correlation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e492747",
      "metadata": {
        "collapsed": true,
        "id": "7e492747"
      },
      "outputs": [],
      "source": [
        "# get data from normalised data stored in gdrive\n",
        "dataframes = pd.read_csv(os.path.join(TIMESERIES_DATA_DIR, \"normalized_data.csv\"), parse_dates=[\"datetime\"])\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "print(f\"\\nTesting directional causality between temp and o3:\")\n",
        "grangercausalitytests(dataframes[['o3', 'temp']], maxlag=4)\n",
        "grangercausalitytests(dataframes[['temp', 'o3']], maxlag=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v5ja2ynKk-uK",
      "metadata": {
        "id": "v5ja2ynKk-uK"
      },
      "source": [
        "p values less than 0.001 in both directions - they both \"Granger-cause\" each other"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mNbsE6DtgmuT",
      "metadata": {
        "id": "mNbsE6DtgmuT"
      },
      "source": [
        "## Dataloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XtHpql78hZA_",
      "metadata": {
        "id": "XtHpql78hZA_"
      },
      "outputs": [],
      "source": [
        "# get data from normalised data stored in gdrive\n",
        "dataframes = pd.read_csv(os.path.join(TIMESERIES_DATA_DIR, \"normalized_data.csv\"), parse_dates=[\"datetime\"])\n",
        "# remove variables other than temp and O3\n",
        "dataframes = dataframes[[\"station_code\", \"datetime\", \"temp\", \"o3\"]]\n",
        "# reducing data size to be able to use in Colab\n",
        "dataframes = dataframes[(dataframes['datetime']>='1997-01-01') & (dataframes['datetime']<='2008-01-01')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55Pwlb1zlheS",
      "metadata": {
        "id": "55Pwlb1zlheS"
      },
      "source": [
        "70/30 train/test split chosen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yjfYe5NYkyIT",
      "metadata": {
        "id": "yjfYe5NYkyIT"
      },
      "outputs": [],
      "source": [
        "# set number of timesteps used as \"past\" values\n",
        "context_window = 336\n",
        "# set number of timesteps used as \"future\" values + also number of values predicted by model later\n",
        "prediction_horizon = 96\n",
        "# variable columns\n",
        "variable_column = [\"temp\", \"o3\"]\n",
        "\n",
        "# create sequences of continuous values\n",
        "X, y = create_sequences(dataframes,variable_column,context_window,prediction_horizon)\n",
        "\n",
        "# perform train/test split of dataset\n",
        "train_size = int(len(X) * 0.7)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# get mean and std of datasets for later denormalisation\n",
        "scaler_stats = {col: {'mean': dataframes[col].mean(), 'std': dataframes[col].std()} for col in variable_column}\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3r04LZ2NpLuH",
      "metadata": {
        "id": "3r04LZ2NpLuH"
      },
      "outputs": [],
      "source": [
        "# checking for hourly gaps\n",
        "df = dataframes.copy()\n",
        "df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "df = df.sort_values(['station_code','datetime'])\n",
        "diffs = df.groupby('station_code')['datetime'].diff()\n",
        "print(diffs.value_counts().head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gu0Onpiwp5yM",
      "metadata": {
        "id": "Gu0Onpiwp5yM"
      },
      "outputs": [],
      "source": [
        "# dropping duplicates\n",
        "dataframes = (\n",
        "    dataframes\n",
        "    .sort_values('datetime')\n",
        "    .drop_duplicates(subset=['datetime'], keep='first')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Pf6Ptp-qUFr",
      "metadata": {
        "id": "9Pf6Ptp-qUFr"
      },
      "outputs": [],
      "source": [
        "# set number of timesteps used as \"past\" values\n",
        "context_window = 336\n",
        "# set number of timesteps used as \"future\" values + also number of values predicted by model later\n",
        "prediction_horizon = 96\n",
        "# variable columns\n",
        "variable_column = [\"temp\", \"o3\"]\n",
        "\n",
        "# create sequences of continuous values\n",
        "X, y = create_sequences(dataframes,variable_column,context_window,prediction_horizon)\n",
        "\n",
        "train_size = int(len(X) * 0.7)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xI7KsPwMrxVc",
      "metadata": {
        "id": "xI7KsPwMrxVc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Drop station_code channel â†’ keep only temp & o3 as inputs\n",
        "X_train_vars = X_train[:, :, 1:1+len(variable_column)].astype(np.float32)\n",
        "X_test_vars  = X_test [:, :, 1:1+len(variable_column)].astype(np.float32)\n",
        "\n",
        "# Pick out only the ozone target from y (o3 is index 1 in variable_column)\n",
        "o3_idx      = variable_column.index(\"o3\")          # find where â€œo3â€ is\n",
        "y_train_o3  = y_train[:, :, 1 + o3_idx].astype(np.float32)  # (4167, 96)\n",
        "y_test_o3   = y_test [:, :, 1 + o3_idx].astype(np.float32)  # (1787, 96)\n",
        "\n",
        "# Now X_train_vars has shape (n,336,2) and y_train_o3 is (n,96)\n",
        "print(f\"X_train_vars shape: {X_train_vars.shape}, y_train_o3 shape: {y_train_o3.shape}\")\n",
        "print(f\"X_test_vars  shape: {X_test_vars.shape},  y_test_o3  shape: {y_test_o3.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyBTapFirFdl",
      "metadata": {
        "id": "tyBTapFirFdl"
      },
      "source": [
        "Not sure if I did the right thing by dropping duplicates just like that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AD8kY_0UuUTc",
      "metadata": {
        "id": "AD8kY_0UuUTc"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GlQoprkeuWib",
      "metadata": {
        "id": "GlQoprkeuWib"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dabbd816",
      "metadata": {
        "id": "dabbd816"
      },
      "outputs": [],
      "source": [
        "# set hyperparameters for the model\n",
        "\n",
        "mlp_hidden_units = [36, 18] # hidden layer sizes\n",
        "mlp_epochs = 50 # number of epochs to train the model\n",
        "mlp_batch_size = 400 # batch size for training\n",
        "activation_fn='relu' # activation function for the hidden layers\n",
        "mlp_optim = 'adam' # optimizer for the model\n",
        "mlp_loss = 'mse' # loss function for the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZMzEEKOXuboF",
      "metadata": {
        "id": "ZMzEEKOXuboF"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Fc9T9Ob9ueI1",
      "metadata": {
        "id": "Fc9T9Ob9ueI1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "# Checkpoint directory for saving models\n",
        "checkpoint_dir = \"./checkpoint/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "num_variables = 1  # We only train one model for ozone forecast\n",
        "\n",
        "mlp_predictions = []\n",
        "training_histories = {}\n",
        "\n",
        "for var_idx in range(num_variables):\n",
        "    print(f\"\\nTraining model for variable {var_idx + 1}/{num_variables}\")\n",
        "\n",
        "    # Flatten multivariate past window: (samples, 336, 2) â†’ (samples, 672)\n",
        "    X_train_single = X_train_vars.reshape(X_train_vars.shape[0], -1)\n",
        "    X_test_single  = X_test_vars.reshape(X_test_vars.shape[0], -1)\n",
        "\n",
        "    # Ozone targets: shape (samples, 96)\n",
        "    y_train_single = y_train_o3.reshape(y_train_o3.shape[0], -1)\n",
        "\n",
        "    # Define model checkpoint file for saving\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"mlp_var{var_idx}.h5\")\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading existing model for variable {var_idx + 1}\")\n",
        "        mlp_model = load_model(checkpoint_path, custom_objects={\"mse\": MeanSquaredError()})\n",
        "    else:\n",
        "        print(f\"Training new model for variable {var_idx + 1}\")\n",
        "\n",
        "        # Build MLP model: input is flattened past (672,), output is 96 future steps\n",
        "        mlp_model = Sequential([\n",
        "            Input(shape=(X_train_single.shape[1],)),\n",
        "            Dense(mlp_hidden_units[0], activation=activation_fn)\n",
        "        ])\n",
        "\n",
        "        # Add additional hidden layers\n",
        "        for units in mlp_hidden_units[1:]:\n",
        "            mlp_model.add(Dense(units, activation=activation_fn))\n",
        "\n",
        "        # Final output layer predicts 96 ozone values\n",
        "        mlp_model.add(Dense(y_train_single.shape[1]))\n",
        "        mlp_model.compile(optimizer=mlp_optim, loss=mlp_loss)\n",
        "\n",
        "        # Save the best model during training\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            checkpoint_path, monitor=\"val_loss\", save_best_only=True, verbose=1\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        training = mlp_model.fit(\n",
        "            X_train_single, y_train_single,\n",
        "            epochs=mlp_epochs, batch_size=mlp_batch_size,\n",
        "            validation_split=0.2, verbose=1,\n",
        "            callbacks=[checkpoint_callback]\n",
        "        )\n",
        "        training_histories[var_idx] = training.history\n",
        "\n",
        "    # Predict future ozone values\n",
        "    y_pred_single = mlp_model.predict(X_test_single.astype(np.float32))\n",
        "    mlp_predictions.append(y_pred_single)\n",
        "\n",
        "# Reshape predictions to (samples, 96, num_variables)\n",
        "mlp_predictions = np.concatenate(mlp_predictions, axis=-1).reshape(y_test_o3.shape[0], prediction_horizon, num_variables)\n",
        "y_pred_single = y_pred_single.reshape(y_pred_single.shape[0], prediction_horizon, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_7EOWfEJzX27",
      "metadata": {
        "id": "_7EOWfEJzX27"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb12fce9",
      "metadata": {
        "id": "cb12fce9"
      },
      "outputs": [],
      "source": [
        "# Reshape ground truth y_test to match the prediction structure: (samples, pred_horizon, num_variables)\n",
        "y_test_reshaped = y_test_o3.reshape(y_test_o3.shape[0], prediction_horizon, num_variables)\n",
        "\n",
        "# Slice out the ozone results (only one page since num_variables=1)\n",
        "y_test_selected = y_test_reshaped[:, :, var_idx]\n",
        "y_pred_selected = mlp_predictions[:, :, var_idx]\n",
        "\n",
        "# Compare predicted vs. true ozone curves for all test samples\n",
        "evaluate_model(y_test_selected, y_pred_selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e6f010",
      "metadata": {
        "id": "15e6f010"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Load ozone model\n",
        "mlp_model = load_model(\n",
        "    os.path.join(checkpoint_dir, \"mlp_var0.h5\"),\n",
        "    custom_objects={\"mse\": MeanSquaredError()}\n",
        ")\n",
        "\n",
        "# Just take first test sample (no station filtering needed if single-station)\n",
        "first_idx = 0\n",
        "\n",
        "# Extract context inputs (temp, o3) and future ozone\n",
        "context_temp = X_test_vars[first_idx, :, 0] * scaler_stats[\"temp\"]['std'] + scaler_stats[\"temp\"]['mean']\n",
        "context_o3   = X_test_vars[first_idx, :, 1] * scaler_stats[\"o3\"]['std'] + scaler_stats[\"o3\"]['mean']\n",
        "\n",
        "actual_future = y_test_o3[first_idx, :] * scaler_stats[\"o3\"]['std'] + scaler_stats[\"o3\"]['mean']\n",
        "\n",
        "# Flatten inputs for model\n",
        "X_flat = X_test_vars[first_idx].reshape(1, -1).astype(np.float32)\n",
        "\n",
        "# Predict future ozone\n",
        "pred_future = mlp_model.predict(X_flat).flatten()\n",
        "predicted_future = pred_future * scaler_stats[\"o3\"]['std'] + scaler_stats[\"o3\"]['mean']\n",
        "\n",
        "# Plot\n",
        "plt.plot(range(context_window), context_temp, label=\"Context Temp\", color=\"lightblue\")\n",
        "plt.plot(range(context_window), context_o3, label=\"Context O3\", color=\"blue\")\n",
        "plt.plot(range(context_window, context_window + prediction_horizon), actual_future, label=\"Actual Future O3\", color=\"green\", marker=\"o\")\n",
        "plt.plot(range(context_window, context_window + prediction_horizon), predicted_future, label=\"MLP Prediction O3\", linestyle=\"--\", color=\"orange\", marker=\"x\")\n",
        "\n",
        "plt.title(\"MLP Forecast for Ozone (First Test Sample)\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save forecast\n",
        "save_forecast(\n",
        "    station=\"DENW094\",\n",
        "    model=\"MLP\",\n",
        "    episode_id=1,\n",
        "    context_vals=context_o3,\n",
        "    future_true=actual_future,\n",
        "    future_pred=predicted_future,\n",
        "    folder=f\"{BASEPATH}/hw_results\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef01dc2",
      "metadata": {
        "id": "0ef01dc2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "64e68c7a",
      "metadata": {
        "id": "64e68c7a"
      },
      "source": [
        "# Observations for Task 2\n",
        "\n",
        "- I was able to generate valid sequences for multivariate forecasting after fixing the dataset.\n",
        "- The issue was duplicate timestamps, which initially caused zero valid windows.\n",
        "- After dropping duplicates per hour, the create_sequences function worked, though I'm not sure this is the best solution.\n",
        "- Later during model training, validation losses were not improving over epochs with the default hyperparameters.\n",
        "- I increased the batch size as I have seen in the literature that a batch size of 10% of the training dataset size is usually used.\n",
        "- I also decreased the hidden units, as I thought the model was overfitting since training losses were improving without improvement (and actually worsening) validation loss. This change in hyperparameters improved validation loss from approx. 0.9 to 0.7.\n",
        "-  However the prediction is still quite poor. I believe dropping duplicates might have caused a shift in the distribution for which we need to normalize again. Therefore we should drop duplicates before any preprocessing steps (in my opinion)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcee8610",
      "metadata": {
        "id": "dcee8610"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}