{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9040d5e5",
      "metadata": {
        "id": "9040d5e5"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maschu09/mless/blob/main/time_series_forecasting/4_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c615ad",
      "metadata": {
        "id": "c8c615ad"
      },
      "source": [
        "# Author: Ayesha Khan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HS4cFVs4ZpYX",
      "metadata": {
        "id": "HS4cFVs4ZpYX"
      },
      "source": [
        "# Table of Contents:\n",
        "\n",
        "  ## 1. Initial Setup\n",
        "  ## 2. Helper Functions\n",
        "  ## 3. Data Download\n",
        "  ## 4. Data Preprocessing\n",
        "  ## 5. Data Preparation for MLP + Granger Analysis\n",
        "  ## 6. Model Setup\n",
        "  ## 7. Training\n",
        "  ## 8. Evaluation\n",
        "  ## 9. Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49eb9d93",
      "metadata": {
        "id": "49eb9d93"
      },
      "source": [
        "# Initial setup, declarations and method definitions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b4eba776",
      "metadata": {
        "id": "b4eba776"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oHVmTJo-yF3z",
      "metadata": {
        "id": "oHVmTJo-yF3z"
      },
      "source": [
        "## Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fc809476",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc809476",
        "outputId": "13459c87-bfd9-4596-d440-12139b55a14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Mount google drive when working in colab\n",
        "hasCOLAB = 'google.colab' in str(get_ipython()) if hasattr(__builtins__,'__IPYTHON__') else False\n",
        "if hasCOLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  BASEPATH = '/content/drive/MyDrive'\n",
        "else:\n",
        "  BASEPATH = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e51ceddc",
      "metadata": {
        "id": "e51ceddc"
      },
      "outputs": [],
      "source": [
        "# Global constants\n",
        "TIMESERIES_DATA_DIR = BASEPATH + \"/timeseries_data/\"\n",
        "# Storage of actual values\n",
        "TIMESERIES_CSV_DIR = os.path.join(TIMESERIES_DATA_DIR, \"toar_csv_timeseries\")\n",
        "# Storage of Timeseries IDs\n",
        "TIMESERIES_ID_FILE = os.path.join(TIMESERIES_DATA_DIR, \"timeseriesIDs.json\")\n",
        "# Min file size for sanity check\n",
        "MIN_FILE_SIZE_BYTES = 100\n",
        "# expected columns from download\n",
        "group_columns = ['station_code', 'latitude', 'longitude']\n",
        "\n",
        "# create directories if not existing\n",
        "os.makedirs(TIMESERIES_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(TIMESERIES_CSV_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9f8684a8",
      "metadata": {
        "id": "9f8684a8"
      },
      "outputs": [],
      "source": [
        "# MLP related imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Input\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# set length of past and future time series\n",
        "# to be used in the model\n",
        "# context_window is the number of past time steps\n",
        "# prediction_horizon is the number of future time steps\n",
        "context_window = 336\n",
        "prediction_horizon = 96"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rDOxC01RXVvy",
      "metadata": {
        "id": "rDOxC01RXVvy"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcwMplcGYFua",
      "metadata": {
        "id": "bcwMplcGYFua"
      },
      "source": [
        "1. load_existing_timeseries,\n",
        "2. save_timeseries_ids,\n",
        "3. fetch_timeseries_data,\n",
        "4. pivot handle,\n",
        "5. csv download,\n",
        "6. evaluate model\n",
        "7. results saver (from task 1)\n",
        "8. fill_six_nans\n",
        "9. log transform if skewed\n",
        "10. standard scaler\n",
        "11. create sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d783afb2",
      "metadata": {
        "id": "d783afb2"
      },
      "outputs": [],
      "source": [
        "def load_existing_timeseries_ids():\n",
        "    \"\"\"\n",
        "    Load existing timeseries IDs from a JSON file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing stored timeseries metadata.\n",
        "    \"\"\"\n",
        "    return json.load(open(TIMESERIES_ID_FILE, 'r')) if os.path.exists(TIMESERIES_ID_FILE) else {}\n",
        "\n",
        "def save_timeseries_ids(timeseries_data):\n",
        "    \"\"\"\n",
        "    Save timeseries metadata to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        timeseries_data (dict): A dictionary containing timeseries metadata.\n",
        "    \"\"\"\n",
        "    json.dump(timeseries_data, open(TIMESERIES_ID_FILE, 'w'), indent=4)\n",
        "\n",
        "def fetch_timeseries_data(station_codes, existing_timeseries, variable_columns):\n",
        "    \"\"\"\n",
        "    Fetch timeseries metadata for given station codes, filtering by specified variables.\n",
        "\n",
        "    Args:\n",
        "        station_codes (list): List of station codes to fetch data for.\n",
        "        existing_timeseries (dict): Dictionary of previously fetched timeseries metadata.\n",
        "        variable_columns (list): List of variable names to retain.\n",
        "\n",
        "    Returns:\n",
        "        dict: Updated dictionary containing filtered timeseries metadata.\n",
        "    \"\"\"\n",
        "    base_url = \"http://toar-data.fz-juelich.de/api/v2/search/?codes=\"\n",
        "    unique_entries = existing_timeseries.copy()\n",
        "    processed_station_codes = {details['station_code'] for details in existing_timeseries.values()}\n",
        "\n",
        "    for code in station_codes:\n",
        "        if code in processed_station_codes:\n",
        "            print(f\"\\t\\tStation {code} is already processed, skipping.\")\n",
        "            continue\n",
        "\n",
        "        response = requests.get(base_url + code, timeout=1000)\n",
        "        if response.status_code == 200:\n",
        "            for entry in response.json():\n",
        "                if (variable_name := entry.get('variable', {}).get('name')) in variable_columns:\n",
        "                    timeseries_id = entry.get('id')\n",
        "                    if timeseries_id not in unique_entries:\n",
        "                        unique_entries[timeseries_id] = {\n",
        "                            'data_start_date': entry.get('data_start_date'),\n",
        "                            'data_end_date': entry.get('data_end_date'),\n",
        "                            'variable_name': variable_name,\n",
        "                            'station_code': code,\n",
        "                            'latitude': entry.get('station', {}).get('coordinates', {}).get('lat'),\n",
        "                            'longitude': entry.get('station', {}).get('coordinates', {}).get('lng'),\n",
        "                        }\n",
        "        else:\n",
        "            print(f\"\\t\\tFailed to fetch data for station {code}. Status code: {response.status_code}\")\n",
        "    return unique_entries\n",
        "\n",
        "def pivot_handle(dfs, metadata_columns, variable_columns):\n",
        "    \"\"\"\n",
        "    Pivot and structure the timeseries dataframe for sequential data analysis.\n",
        "\n",
        "    Args:\n",
        "        dfs (pd.DataFrame): Dataframe containing timeseries data.\n",
        "        metadata_columns (list): List of metadata column names.\n",
        "        variable_columns (list): List of variable names to include.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed dataframe with pivoted structure.\n",
        "    \"\"\"\n",
        "    pivot_df = dfs.pivot_table(index='datetime', columns='variable_name', values='value', aggfunc='mean')\n",
        "    pivot_df.reset_index(inplace=True)\n",
        "\n",
        "    print(f\"Station {dfs['station_code'].unique()} min time: {dfs['datetime'].min()}, max time: {dfs['datetime'].max()}, hours between: {(dfs['datetime'].max() - dfs['datetime'].min()) / pd.Timedelta(hours=1):.2f}\")\n",
        "    reference_index = pd.date_range(start=dfs['datetime'].min(), end=dfs['datetime'].max(), freq=\"h\", tz=\"UTC\")\n",
        "    reference_df = pd.DataFrame({'datetime': reference_index})\n",
        "\n",
        "    pivot_df_ = reference_df.merge(pivot_df, on='datetime', how='left')\n",
        "\n",
        "    for col in metadata_columns:\n",
        "        if dfs[col].notna().any():\n",
        "            value = dfs[col].dropna().iloc[0]\n",
        "            pivot_df_.insert(0, col, value)\n",
        "        else:\n",
        "            print(f\"Station {dfs['station_code'].unique()}: metadata {col} has no value\")\n",
        "\n",
        "    return pivot_df_\n",
        "\n",
        "def download_csv_data(timeseries_data, variable_columns):\n",
        "    \"\"\"\n",
        "    Download and process CSV data for each timeseries ID.\n",
        "\n",
        "    Args:\n",
        "        timeseries_data (dict): Dictionary containing timeseries metadata.\n",
        "        variable_columns (list): List of variable names to process.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Combined dataframe of all timeseries data.\n",
        "    \"\"\"\n",
        "    dataframes = []\n",
        "    metadata_columns = ['station_code', 'latitude', 'longitude']\n",
        "\n",
        "    for ts_id, details in timeseries_data.items():\n",
        "        csv_path = os.path.join(TIMESERIES_CSV_DIR, f\"{ts_id}.csv\")\n",
        "\n",
        "        if os.path.exists(csv_path) and os.path.getsize(csv_path) > MIN_FILE_SIZE_BYTES:\n",
        "            print(f\"\\tCSV already exists for timeseries ID {ts_id}, skipping download.\")\n",
        "        else:\n",
        "            print(f\"\\tDownloading data for timeseries ID {ts_id}\")\n",
        "            url = f\"http://toar-data.fz-juelich.de/api/v2/data/timeseries/{ts_id}?format=csv\"\n",
        "            try:\n",
        "                response = requests.get(url, stream=True, timeout=1000)\n",
        "                response.raise_for_status()\n",
        "                with open(csv_path, 'wb') as file:\n",
        "                    file.writelines(response.iter_content(chunk_size=8192))\n",
        "                print(f\"\\t\\tRaw data CSV of {ts_id} saved: {csv_path}\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"\\t\\tFailed to download data for timeseries ID {ts_id}. Error: {e}\")\n",
        "                continue\n",
        "\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path, skiprows=lambda i: i < next(i for i, line in enumerate(open(csv_path)) if line.startswith('datetime')), low_memory=False)\n",
        "            df['datetime'] = pd.to_datetime(df['datetime'], format='mixed')\n",
        "            df[['variable_name', 'station_code', 'latitude', 'longitude']] = details['variable_name'], details['station_code'], details['latitude'], details['longitude']\n",
        "            print(f\"Dataframe for timeseries ID {ts_id} loaded successfully with shape {df.shape}\")\n",
        "            dataframes.append(pivot_handle(df, metadata_columns, variable_columns))\n",
        "        except (pd.errors.EmptyDataError, pd.errors.ParserError) as e:\n",
        "            print(f\"\\tError processing CSV for timeseries ID {ts_id}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return pd.concat(dataframes, ignore_index=True).sort_values(by=['station_code', 'datetime']) if dataframes else pd.DataFrame()\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4adeb9ea",
      "metadata": {
        "id": "4adeb9ea"
      },
      "outputs": [],
      "source": [
        "# function to save forecast results in a structured format\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "def save_forecast(\n",
        "        station: str,\n",
        "        model: str,\n",
        "        context_vals,               # 1-D array-like, X test values\n",
        "        future_true,                # 1-D array-like, true future values\n",
        "        future_pred,                # 1-D array-like, predicted future values\n",
        "        episode_id,                 # int index, to identify the timeperiod\n",
        "        folder=f\"{BASEPATH}/results\"): # default folder for results\n",
        "    \"\"\"\n",
        "    One call per model-run. Stores just enough metadata to let the\n",
        "    plotting notebook know what it’s looking at.\n",
        "    \"\"\"\n",
        "    import os, json, numpy as np\n",
        "    # make sure the base path exists\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    # structure the result\n",
        "    # note: context_vals, future_true, and future_pred should be 1-D arrays\n",
        "    result = {\n",
        "        \"station\":        station,              # <- allows filtering\n",
        "        \"episode_id\":     episode_id,           # <- allows filtering\n",
        "        \"model\":          model,                # <- allows choosing of model\n",
        "        \"context\":        np.asarray(context_vals).tolist(),    # converts to numpy array and then to list\n",
        "        \"future_true\":    np.asarray(future_true).tolist(),\n",
        "        \"future_pred\":    np.asarray(future_pred).tolist()\n",
        "    }\n",
        "\n",
        "    # create a filename based on model, station, and episode_id\n",
        "    fname = f\"{model}_{station}_{episode_id}.json\"\n",
        "\n",
        "    # save the result to a JSON file\n",
        "    with open(os.path.join(folder, fname), \"w\") as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "\n",
        "    # let user know station code and episode_id for plotting purpose\n",
        "    print(f\"Results saved as {fname}, episode_id is {episode_id} and station is {station}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3uQAB9LXON5j",
      "metadata": {
        "id": "3uQAB9LXON5j"
      },
      "outputs": [],
      "source": [
        "\n",
        "def fill_six_nans(group):\n",
        "    \"\"\"\n",
        "    Fills up to six consecutive NaN values in a given pandas Series using linear interpolation\n",
        "    if the NaNs are surrounded by valid values. If the NaNs are at the start, they are replaced\n",
        "    with zeros, and if they are at the end, they are filled with the last known value.\n",
        "\n",
        "    Args:\n",
        "        group (pd.Series): The input Series with potential NaN values.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A Series where up to six consecutive NaNs are interpolated, and longer NaN\n",
        "        sequences are partially filled while preserving the original index.\n",
        "    \"\"\"\n",
        "    values = group.to_numpy()\n",
        "    i = 0\n",
        "    while i < len(values):\n",
        "        if np.isnan(values[i]):\n",
        "            start = i\n",
        "            while i < len(values) and np.isnan(values[i]):\n",
        "                i += 1\n",
        "            end = min(i, start + 6)  # Limit to filling only 6 NaNs\n",
        "\n",
        "            if start > 0 and i < len(values):  # NaNs in the middle\n",
        "                fill_values = np.linspace(values[start - 1], values[i], end - start + 2)[1:-1]\n",
        "            elif start == 0:  # NaNs at the start\n",
        "                fill_values = [0] * (end - start)\n",
        "            elif i >= len(values):  # NaNs at the end\n",
        "                fill_values = [values[start - 1]] * (end - start)\n",
        "            values[start:end] = fill_values\n",
        "        else:\n",
        "            i += 1\n",
        "    return pd.Series(values, index=group.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "B5RHWNiheMYD",
      "metadata": {
        "id": "B5RHWNiheMYD"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import skew\n",
        "\n",
        "def log_transform_if_skewed(df, columns, threshold=1.0):\n",
        "    \"\"\"\n",
        "    Log-transform the specified columns of a DataFrame based on their skewdness.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        columns (list): List of column names that need to be checked for skewdness.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with normalized columns.\n",
        "    \"\"\"\n",
        "    df_transformed = df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        # s = df[col].dropna()\n",
        "        s = df[col]\n",
        "        current_skewness = skew(s)\n",
        "\n",
        "        print(f\"[{col}] Skewness: {current_skewness:.2f}\")\n",
        "\n",
        "        if abs(current_skewness) > threshold:\n",
        "            # To avoid log(0) or log(negative values).\n",
        "            if (s <= 0).any():\n",
        "                shift = abs(s.min()) + 1e-6\n",
        "                print(f\"Applying log(x + {shift:.6f}) to {col}\")\n",
        "                df_transformed[col] = np.log(df[col] + shift)\n",
        "            else:\n",
        "                print(f\"Applying log(x) to {col}\")\n",
        "                df_transformed[col] = np.log(df[col])\n",
        "        else:\n",
        "            print(f\"No transformation applied to {col}.\")\n",
        "\n",
        "    return df_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "VbQtenve606u",
      "metadata": {
        "id": "VbQtenve606u"
      },
      "outputs": [],
      "source": [
        "def standard_scaler(df, columns):\n",
        "    \"\"\"\n",
        "    Standardize the specified columns of a DataFrame by subtracting the mean\n",
        "    and dividing by the standard deviation (Z-score normalization).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        columns (list): List of column names to be normalized.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with normalized columns.\n",
        "    \"\"\"\n",
        "    df_scaled = df.copy()\n",
        "    for col in columns:\n",
        "        mean = df_scaled[col].mean()\n",
        "        std = df_scaled[col].std()\n",
        "        df_scaled[col] = (df_scaled[col] - mean) / std\n",
        "    return df_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UJKeh1FneL-x",
      "metadata": {
        "id": "UJKeh1FneL-x"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "TynS8Wxnl8vY",
      "metadata": {
        "id": "TynS8Wxnl8vY"
      },
      "outputs": [],
      "source": [
        "# Function to create continous time-series data with past 24 hours as input and next 6 hours as output\n",
        "def create_sequences(data, variable_column, n_past=336, n_future=96):\n",
        "    df = data.copy()\n",
        "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "    df = df.sort_values(['station_code', 'datetime']).reset_index(drop=True)\n",
        "\n",
        "    # 1) Encode station_code as a small integer (categorical codes) to save memory\n",
        "    df['sc_code'] = df['station_code'].astype('category')\n",
        "\n",
        "    # 2) Boolean mask: True if exactly 1 hour after the previous row (per station)\n",
        "    is1h = (\n",
        "        df.groupby('station_code')['datetime']\n",
        "          .diff()\n",
        "          .eq(pd.Timedelta(hours=1))\n",
        "    )\n",
        "\n",
        "    # 3) Compute how many consecutive “True” over (n_past + n_future - 1) rows\n",
        "    gap_window = n_past + n_future - 1\n",
        "    run = (\n",
        "        is1h.groupby(df['station_code'])\n",
        "            .rolling(window=gap_window, min_periods=gap_window)\n",
        "            .sum()\n",
        "            .reset_index(level=0, drop=True)\n",
        "    )\n",
        "\n",
        "    # 4) 'ends' are indices where run == gap_window (end of a full-length continuous block)\n",
        "    ends = run[run == gap_window].index\n",
        "\n",
        "    # 5) Grab column‐arrays for encoded station code and the variable\n",
        "    sc_arr  = df['sc_code'].values\n",
        "    vals    = df[variable_column].values\n",
        "\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "    for end_idx in ends:\n",
        "        start_idx = end_idx - gap_window\n",
        "        if start_idx >= 0 and sc_arr[start_idx] == sc_arr[end_idx]:\n",
        "            station_block = sc_arr[start_idx : end_idx + 1]\n",
        "            var_block     = vals[start_idx : end_idx + 1]\n",
        "\n",
        "            block = np.column_stack((station_block, var_block))\n",
        "\n",
        "            X_list.append(block[:n_past, :])\n",
        "            y_list.append(block[n_past:, :])\n",
        "\n",
        "    if not X_list:\n",
        "        # No valid windows\n",
        "        return np.empty((0, n_past, 2)), np.empty((0, n_future, 2))\n",
        "\n",
        "    X = np.stack(X_list)  # shape = (num_windows, n_past, 2)\n",
        "    y = np.stack(y_list)  # shape = (num_windows, n_future, 2)\n",
        "    return X, y\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8_I6FbZMYmqO",
      "metadata": {
        "id": "8_I6FbZMYmqO"
      },
      "source": [
        "# Data Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fNDdjHgyXvMz",
      "metadata": {
        "id": "fNDdjHgyXvMz"
      },
      "outputs": [],
      "source": [
        "# select station for download\n",
        "station_codes = [\"DENW094\"]\n",
        "# select variables for download\n",
        "variable_columns = [\"temp\", \"o3\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xAc3wxr3Y982",
      "metadata": {
        "id": "xAc3wxr3Y982"
      },
      "source": [
        "## Download via REST API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "bIELzP2mY9gz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIELzP2mY9gz",
        "outputId": "848fbe06-f361-403f-9e7d-6ae23ec594f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\n",
            "Variables available for DENW094: ['o3', 'no2', 'no', 'temp', 'wspeed', 'wdir', 'pm10', 'press', 'cloudcover', 'u']\n",
            "\t Number of time series meta data fetched : 2\n",
            "\tDownloading data for timeseries ID 73\n",
            "\t\tFailed to download data for timeseries ID 73. Error: 401 Client Error: Unauthorized for url: https://toar-data.fz-juelich.de/api/v2/data/timeseries/73?format=csv\n",
            "\tDownloading data for timeseries ID 76\n",
            "\t\tFailed to download data for timeseries ID 76. Error: 401 Client Error: Unauthorized for url: https://toar-data.fz-juelich.de/api/v2/data/timeseries/76?format=csv\n",
            "\t Total dataFrames processed : 0 and shape of first dataframe (0, 0).\n"
          ]
        }
      ],
      "source": [
        "# Load existing timeseries IDs from json to skip calls to TOAR\n",
        "existing_timeseries = load_existing_timeseries_ids()\n",
        "\n",
        "# download timeseries IDs required\n",
        "timeseries_data = fetch_timeseries_data(station_codes, existing_timeseries, variable_columns)\n",
        "print(f\"\\t Number of time series meta data fetched : {len(timeseries_data)}\")\n",
        "\n",
        "# save existing timeseries IDs as json to reduce calls to TOAR in future\n",
        "save_timeseries_ids(timeseries_data)\n",
        "\n",
        "# download actual data and make into dataframe\n",
        "dataframes = download_csv_data(timeseries_data,variable_columns)\n",
        "print(f\"\\t Total dataFrames processed : {len(dataframes)} and shape of first dataframe {dataframes.shape}.\")\n",
        "\n",
        "dataframes.head()\n",
        "\n",
        "# save data in case session timeout\n",
        "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F6MRdgvscWTS",
      "metadata": {
        "id": "F6MRdgvscWTS"
      },
      "outputs": [],
      "source": [
        "# save data in case session timeout\n",
        "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VPF_l5McczAy",
      "metadata": {
        "id": "VPF_l5McczAy"
      },
      "source": [
        "Code in case of timeout to load data again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dB3k8jBCc0O8",
      "metadata": {
        "id": "dB3k8jBCc0O8"
      },
      "outputs": [],
      "source": [
        "# save data path variable\n",
        "data_path = os.path.join(TIMESERIES_DATA_DIR, \"raw_data.csv\")\n",
        "# read in raw data\n",
        "dataframes = pd.read_csv(data_path,parse_dates=[\"datetime\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GgDSGws-Zm-e",
      "metadata": {
        "id": "GgDSGws-Zm-e"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QTh-HCFhaWq9",
      "metadata": {
        "id": "QTh-HCFhaWq9"
      },
      "source": [
        "## Dealing with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "mA_p0yV0aZIO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "mA_p0yV0aZIO",
        "outputId": "b8bc2b8f-3a9d-46bc-eada-d03c95e51e55"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Columns not found: 'o3'\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-49559073.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# groups values by variable and fills in six NaN values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_columns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_six_nans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# checks number of na values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# drop remaining na values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1949\u001b[0m                 \u001b[0;34m\"Use a list instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m             )\n\u001b[0;32m-> 1951\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mbad_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Columns not found: {str(bad_keys)[1:-1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Columns not found: 'o3'\""
          ]
        }
      ],
      "source": [
        "# groups values by variable and fills in six NaN values\n",
        "dataframes[variable_columns] = dataframes.groupby(group_columns)[variable_columns].transform(fill_six_nans)\n",
        "# checks number of na values\n",
        "dataframes.isna().sum()\n",
        "# drop remaining na values\n",
        "dataframes = dataframes.dropna()\n",
        "# check again\n",
        "dataframes.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qd4YM9V0Smu8",
      "metadata": {
        "id": "qd4YM9V0Smu8"
      },
      "source": [
        "## Statistical Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61S7XC7zNU1y",
      "metadata": {
        "id": "61S7XC7zNU1y"
      },
      "outputs": [],
      "source": [
        "stats = ['min', 'max', 'mean', 'sum', 'std', 'var', 'median','prod','nunique',\n",
        "    ('5th_percentile', lambda x: x.quantile(0.05)),\n",
        "    ('10th_percentile', lambda x: x.quantile(0.10)),\n",
        "    ('25th_percentile', lambda x: x.quantile(0.25)),\n",
        "    ('50th_percentile', lambda x: x.quantile(0.50)), #(median)\n",
        "    ('75th_percentile', lambda x: x.quantile(0.75))]\n",
        "agg_dict = {col: stats for col in variable_columns}\n",
        "grouped = dataframes.groupby('station_code').agg(agg_dict)\n",
        "display(grouped)\n",
        "\n",
        "for agg_func in ['min', 'max', 'mean', 'std']:\n",
        "    display(agg_func)\n",
        "    agg_view = grouped.xs(agg_func, axis=1, level=1)\n",
        "    display(agg_view)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vO5hGVrwehyz",
      "metadata": {
        "id": "vO5hGVrwehyz"
      },
      "source": [
        "# Data Preparation for MLP + Granger Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bUkal4KWgRc9",
      "metadata": {
        "id": "bUkal4KWgRc9"
      },
      "source": [
        "## Improving ditribution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N7BFnIL-eQZ6",
      "metadata": {
        "id": "N7BFnIL-eQZ6"
      },
      "source": [
        "### Log Scaling after checking for skewdness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cxGV6_LeXLz",
      "metadata": {
        "id": "6cxGV6_LeXLz"
      },
      "outputs": [],
      "source": [
        "# log transofmr data if distribution is not gaussian\n",
        "dataframe_= log_transform_if_skewed(dataframes, variable_columns, threshold=1.0)\n",
        "dataframe_.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nog3ce9heoBq",
      "metadata": {
        "id": "nog3ce9heoBq"
      },
      "source": [
        "### Normalise the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "emmHuMtpfC2T",
      "metadata": {
        "id": "emmHuMtpfC2T"
      },
      "outputs": [],
      "source": [
        "# z-normalisation of data\n",
        "dataframes = standard_scaler(dataframe_, variable_columns)\n",
        "# show dataframe\n",
        "dataframes.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-cc-ixdbc_QR",
      "metadata": {
        "id": "-cc-ixdbc_QR"
      },
      "source": [
        "##### Save the normalized dataframe for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Oi3PPzzJZ5Vf",
      "metadata": {
        "id": "Oi3PPzzJZ5Vf"
      },
      "outputs": [],
      "source": [
        "dataframes.to_csv(os.path.join(TIMESERIES_DATA_DIR, \"normalized_data.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d6c8376",
      "metadata": {
        "id": "7d6c8376"
      },
      "source": [
        "## Granger Analysis\n",
        "\n",
        "😈 **Task 3:** Run a Granger test between `temp` and `o3`. Is there any directional causality?\n",
        "\n",
        "😈 **Question 3:** Why is Granger causality not the same as actual causality?\n",
        "Granger causality only checks if it is possible to predict the relationship between two series; it does not establish the actual causality. So it could be only correlation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7e492747",
      "metadata": {
        "collapsed": true,
        "id": "7e492747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14dcd89d-80ee-4e9c-e6e6-c2de1c58306d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing directional causality between temp and o3:\n",
            "\n",
            "Granger Causality\n",
            "number of lags (no zero) 1\n",
            "ssr based F test:         F=126.7824, p=0.0000  , df_denom=700842, df_num=1\n",
            "ssr based chi2 test:   chi2=126.7830, p=0.0000  , df=1\n",
            "likelihood ratio test: chi2=126.7715, p=0.0000  , df=1\n",
            "parameter F test:         F=126.7824, p=0.0000  , df_denom=700842, df_num=1\n",
            "\n",
            "Granger Causality\n",
            "number of lags (no zero) 2\n",
            "ssr based F test:         F=1479.4247, p=0.0000  , df_denom=700839, df_num=2\n",
            "ssr based chi2 test:   chi2=2958.8705, p=0.0000  , df=2\n",
            "likelihood ratio test: chi2=2952.6420, p=0.0000  , df=2\n",
            "parameter F test:         F=1479.4247, p=0.0000  , df_denom=700839, df_num=2\n",
            "\n",
            "Granger Causality\n",
            "number of lags (no zero) 3\n",
            "ssr based F test:         F=2517.0234, p=0.0000  , df_denom=700836, df_num=3\n",
            "ssr based chi2 test:   chi2=7551.1455, p=0.0000  , df=3\n",
            "likelihood ratio test: chi2=7510.7560, p=0.0000  , df=3\n",
            "parameter F test:         F=2517.0234, p=0.0000  , df_denom=700836, df_num=3\n",
            "\n",
            "Granger Causality\n",
            "number of lags (no zero) 4\n",
            "ssr based F test:         F=3522.0736, p=0.0000  , df_denom=700833, df_num=4\n",
            "ssr based chi2 test:   chi2=14088.4754, p=0.0000  , df=4\n",
            "likelihood ratio test: chi2=13948.7402, p=0.0000  , df=4\n",
            "parameter F test:         F=3522.0736, p=0.0000  , df_denom=700833, df_num=4\n",
            "\n",
            "Granger Causality\n",
            "number of lags (no zero) 1\n",
            "ssr based F test:         F=1123.4460, p=0.0000  , df_denom=700842, df_num=1\n",
            "ssr based chi2 test:   chi2=1123.4508, p=0.0000  , df=1\n",
            "likelihood ratio test: chi2=1122.5514, p=0.0000  , df=1\n",
            "parameter F test:         F=1123.4460, p=0.0000  , df_denom=700842, df_num=1\n",
            "\n",
            "Granger Causality\n",
            "number of lags (no zero) 2\n",
            "ssr based F test:         F=3125.7956, p=0.0000  , df_denom=700839, df_num=2\n",
            "ssr based chi2 test:   chi2=6251.6359, p=0.0000  , df=2\n",
            "likelihood ratio test: chi2=6223.9178, p=0.0000  , df=2\n",
            "parameter F test:         F=3125.7956, p=0.0000  , df_denom=700839, df_num=2\n",
            "\n",
            "Granger Causality\n",
            "number of lags (no zero) 3\n",
            "ssr based F test:         F=4269.0687, p=0.0000  , df_denom=700836, df_num=3\n",
            "ssr based chi2 test:   chi2=12807.3339, p=0.0000  , df=3\n",
            "likelihood ratio test: chi2=12691.7185, p=0.0000  , df=3\n",
            "parameter F test:         F=4269.0687, p=0.0000  , df_denom=700836, df_num=3\n",
            "\n",
            "Granger Causality\n",
            "number of lags (no zero) 4\n",
            "ssr based F test:         F=4834.2362, p=0.0000  , df_denom=700833, df_num=4\n",
            "ssr based chi2 test:   chi2=19337.1931, p=0.0000  , df=4\n",
            "likelihood ratio test: chi2=19075.2309, p=0.0000  , df=4\n",
            "parameter F test:         F=4834.2362, p=0.0000  , df_denom=700833, df_num=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{np.int64(1): ({'ssr_ftest': (np.float64(1123.446028097791),\n",
              "    np.float64(4.156613845185853e-246),\n",
              "    np.float64(700842.0),\n",
              "    np.int64(1)),\n",
              "   'ssr_chi2test': (np.float64(1123.450837081962),\n",
              "    np.float64(2.6425772564189183e-246),\n",
              "    np.int64(1)),\n",
              "   'lrtest': (np.float64(1122.5513553153723),\n",
              "    np.float64(4.144969079843009e-246),\n",
              "    np.int64(1)),\n",
              "   'params_ftest': (np.float64(1123.446028097952),\n",
              "    np.float64(4.156613844698179e-246),\n",
              "    np.float64(700842.0),\n",
              "    1.0)},\n",
              "  [<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x79e90db1a450>,\n",
              "   <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x79e8a76d1b50>,\n",
              "   array([[0., 1., 0.]])]),\n",
              " np.int64(2): ({'ssr_ftest': (np.float64(3125.7956319505156),\n",
              "    np.float64(0.0),\n",
              "    np.float64(700839.0),\n",
              "    np.int64(2)),\n",
              "   'ssr_chi2test': (np.float64(6251.635864667141),\n",
              "    np.float64(0.0),\n",
              "    np.int64(2)),\n",
              "   'lrtest': (np.float64(6223.917800734751), np.float64(0.0), np.int64(2)),\n",
              "   'params_ftest': (np.float64(3125.7956319494133),\n",
              "    np.float64(0.0),\n",
              "    np.float64(700839.0),\n",
              "    2.0)},\n",
              "  [<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x79e8a76b3210>,\n",
              "   <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x79e90dafc710>,\n",
              "   array([[0., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 1., 0.]])]),\n",
              " np.int64(3): ({'ssr_ftest': (np.float64(4269.068659777021),\n",
              "    np.float64(0.0),\n",
              "    np.float64(700836.0),\n",
              "    np.int64(3)),\n",
              "   'ssr_chi2test': (np.float64(12807.333898618677),\n",
              "    np.float64(0.0),\n",
              "    np.int64(3)),\n",
              "   'lrtest': (np.float64(12691.718504551798), np.float64(0.0), np.int64(3)),\n",
              "   'params_ftest': (np.float64(4269.068659777458),\n",
              "    np.float64(0.0),\n",
              "    np.float64(700836.0),\n",
              "    3.0)},\n",
              "  [<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x79e90dce13d0>,\n",
              "   <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x79e90dce19d0>,\n",
              "   array([[0., 0., 0., 1., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 1., 0.]])]),\n",
              " np.int64(4): ({'ssr_ftest': (np.float64(4834.236183618889),\n",
              "    np.float64(0.0),\n",
              "    np.float64(700833.0),\n",
              "    np.int64(4)),\n",
              "   'ssr_chi2test': (np.float64(19337.193056832824),\n",
              "    np.float64(0.0),\n",
              "    np.int64(4)),\n",
              "   'lrtest': (np.float64(19075.2308816202), np.float64(0.0), np.int64(4)),\n",
              "   'params_ftest': (np.float64(4834.236183617633),\n",
              "    np.float64(0.0),\n",
              "    np.float64(700833.0),\n",
              "    4.0)},\n",
              "  [<statsmodels.regression.linear_model.RegressionResultsWrapper at 0x79e90db6d5d0>,\n",
              "   <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x79e90daf0110>,\n",
              "   array([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "          [0., 0., 0., 0., 0., 0., 0., 1., 0.]])])}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# get data from normalised data stored in gdrive\n",
        "dataframes = pd.read_csv(os.path.join(TIMESERIES_DATA_DIR, \"normalized_data.csv\"), parse_dates=[\"datetime\"])\n",
        "# below url given in 1_download... notebook\n",
        "# url = \"https://drive.google.com/uc?export=download&id=16Mjahl_vSznbXUFeD80xNbsz2eFmCeLG\"\n",
        "# dataframes = pd.read_csv(url)\n",
        "from statsmodels.tsa.stattools import grangercausalitytests\n",
        "print(f\"\\nTesting directional causality between temp and o3:\")\n",
        "grangercausalitytests(dataframes[['o3', 'temp']], maxlag=4)\n",
        "grangercausalitytests(dataframes[['temp', 'o3']], maxlag=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v5ja2ynKk-uK",
      "metadata": {
        "id": "v5ja2ynKk-uK"
      },
      "source": [
        "p values less than 0.001 in both directions - they both \"Granger-cause\" each other"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mNbsE6DtgmuT",
      "metadata": {
        "id": "mNbsE6DtgmuT"
      },
      "source": [
        "## Dataloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "XtHpql78hZA_",
      "metadata": {
        "id": "XtHpql78hZA_"
      },
      "outputs": [],
      "source": [
        "# get data from normalised data stored in gdrive\n",
        "dataframes = pd.read_csv(os.path.join(TIMESERIES_DATA_DIR, \"normalized_data.csv\"), parse_dates=[\"datetime\"])\n",
        "# remove variables other than temp and O3\n",
        "dataframes = dataframes[[\"station_code\", \"datetime\", \"temp\", \"o3\"]]\n",
        "# reducing data size to be able to use in Colab\n",
        "dataframes = dataframes[(dataframes['datetime']>='1997-01-01') & (dataframes['datetime']<='2008-01-01')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55Pwlb1zlheS",
      "metadata": {
        "id": "55Pwlb1zlheS"
      },
      "source": [
        "70/30 train/test split chosen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "yjfYe5NYkyIT",
      "metadata": {
        "id": "yjfYe5NYkyIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fb0c83-8f40-4a8c-88df-ba10f35eee1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (0, 336, 2), y_train shape: (0, 96, 2)\n",
            "X_test shape: (0, 336, 2), y_test shape: (0, 96, 2)\n"
          ]
        }
      ],
      "source": [
        "# set number of timesteps used as \"past\" values\n",
        "context_window = 336\n",
        "# set number of timesteps used as \"future\" values + also number of values predicted by model later\n",
        "prediction_horizon = 96\n",
        "# variable columns\n",
        "variable_column = [\"temp\", \"o3\"]\n",
        "\n",
        "# create sequences of continuous values\n",
        "X, y = create_sequences(dataframes,variable_column,context_window,prediction_horizon)\n",
        "\n",
        "# perform train/test split of dataset\n",
        "train_size = int(len(X) * 0.7)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# get mean and std of datasets for later denormalisation\n",
        "scaler_stats = {col: {'mean': dataframes[col].mean(), 'std': dataframes[col].std()} for col in variable_column}\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No samples found."
      ],
      "metadata": {
        "id": "HF1nsq89nR5w"
      },
      "id": "HF1nsq89nR5w"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3r04LZ2NpLuH",
      "metadata": {
        "id": "3r04LZ2NpLuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a71ba6f5-c030-4fe3-a6ad-1faf5335d01c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datetime\n",
            "0 days 00:00:00    182093\n",
            "0 days 01:00:00     48684\n",
            "0 days 02:00:00       133\n",
            "0 days 04:00:00        42\n",
            "0 days 03:00:00        36\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# checking for hourly gaps\n",
        "df = dataframes.copy()\n",
        "df['datetime'] = pd.to_datetime(df['datetime'])\n",
        "df = df.sort_values(['station_code','datetime'])\n",
        "diffs = df.groupby('station_code')['datetime'].diff()\n",
        "print(diffs.value_counts().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping duplicates to create valid sequences."
      ],
      "metadata": {
        "id": "wQOtZcy8nZle"
      },
      "id": "wQOtZcy8nZle"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "Gu0Onpiwp5yM",
      "metadata": {
        "id": "Gu0Onpiwp5yM"
      },
      "outputs": [],
      "source": [
        "# dropping duplicates\n",
        "dataframes = (\n",
        "    dataframes\n",
        "    .sort_values('datetime')\n",
        "    .drop_duplicates(subset=['datetime'], keep='first')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9Pf6Ptp-qUFr",
      "metadata": {
        "id": "9Pf6Ptp-qUFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b156a6-dc72-4c47-dd26-a31947bae96e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (4167, 336, 3), y_train shape: (4167, 96, 3)\n",
            "X_test shape: (1787, 336, 3), y_test shape: (1787, 96, 3)\n"
          ]
        }
      ],
      "source": [
        "# set number of timesteps used as \"past\" values\n",
        "context_window = 336\n",
        "# set number of timesteps used as \"future\" values + also number of values predicted by model later\n",
        "prediction_horizon = 96\n",
        "# variable columns\n",
        "variable_column = [\"temp\", \"o3\"]\n",
        "\n",
        "# create sequences of continuous values\n",
        "X, y = create_sequences(dataframes,variable_column,context_window,prediction_horizon)\n",
        "\n",
        "train_size = int(len(X) * 0.7)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "xI7KsPwMrxVc",
      "metadata": {
        "id": "xI7KsPwMrxVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe716937-2bef-4dc2-c10e-e0d47c1b4d1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train_vars shape: (4167, 336, 2), y_train_o3 shape: (4167, 96)\n",
            "X_test_vars  shape: (1787, 336, 2),  y_test_o3  shape: (1787, 96)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Drop station_code channel → keep only temp & o3 as inputs\n",
        "X_train_vars = X_train[:, :, 1:1+len(variable_column)].astype(np.float32)\n",
        "X_test_vars  = X_test [:, :, 1:1+len(variable_column)].astype(np.float32)\n",
        "\n",
        "# Pick out only the ozone target from y (o3 is index 1 in variable_column)\n",
        "o3_idx      = variable_column.index(\"o3\")          # find where “o3” is\n",
        "y_train_o3  = y_train[:, :, 1 + o3_idx].astype(np.float32)  # (4167, 96)\n",
        "y_test_o3   = y_test [:, :, 1 + o3_idx].astype(np.float32)  # (1787, 96)\n",
        "\n",
        "# Now X_train_vars has shape (n,336,2) and y_train_o3 is (n,96)\n",
        "print(f\"X_train_vars shape: {X_train_vars.shape}, y_train_o3 shape: {y_train_o3.shape}\")\n",
        "print(f\"X_test_vars  shape: {X_test_vars.shape},  y_test_o3  shape: {y_test_o3.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tyBTapFirFdl",
      "metadata": {
        "id": "tyBTapFirFdl"
      },
      "source": [
        "Not sure if I did the right thing by dropping duplicates just like that."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AD8kY_0UuUTc",
      "metadata": {
        "id": "AD8kY_0UuUTc"
      },
      "source": [
        "# Model Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GlQoprkeuWib",
      "metadata": {
        "id": "GlQoprkeuWib"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "dabbd816",
      "metadata": {
        "id": "dabbd816"
      },
      "outputs": [],
      "source": [
        "# set hyperparameters for the model\n",
        "\n",
        "mlp_hidden_units = [36, 18] # hidden layer sizes\n",
        "mlp_epochs = 50 # number of epochs to train the model\n",
        "mlp_batch_size = 400 # batch size for training\n",
        "activation_fn='relu' # activation function for the hidden layers\n",
        "mlp_optim = 'adam' # optimizer for the model\n",
        "mlp_loss = 'mse' # loss function for the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZMzEEKOXuboF",
      "metadata": {
        "id": "ZMzEEKOXuboF"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "Fc9T9Ob9ueI1",
      "metadata": {
        "id": "Fc9T9Ob9ueI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee3e13b-82ed-4e3c-e044-ceb3eb2a3961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training model for variable 1/1\n",
            "Training new model for variable 1\n",
            "Epoch 1/50\n",
            "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.4797\n",
            "Epoch 1: val_loss improved from inf to 1.40520, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: 1.4701 - val_loss: 1.4052\n",
            "Epoch 2/50\n",
            "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3852  \n",
            "Epoch 2: val_loss improved from 1.40520 to 1.37527, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.3783 - val_loss: 1.3753\n",
            "Epoch 3/50\n",
            "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.3029 \n",
            "Epoch 3: val_loss improved from 1.37527 to 1.34985, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.2988 - val_loss: 1.3499\n",
            "Epoch 4/50\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1.2024 \n",
            "Epoch 4: val_loss improved from 1.34985 to 1.34599, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.2021 - val_loss: 1.3460\n",
            "Epoch 5/50\n",
            "\u001b[1m8/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1.1312 \n",
            "Epoch 5: val_loss did not improve from 1.34599\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1280 - val_loss: 1.3497\n",
            "Epoch 6/50\n",
            "\u001b[1m5/9\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.0281\n",
            "Epoch 6: val_loss improved from 1.34599 to 1.33851, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.0322 - val_loss: 1.3385\n",
            "Epoch 7/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 344ms/step - loss: 0.9571\n",
            "Epoch 7: val_loss improved from 1.33851 to 1.33121, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.9612 - val_loss: 1.3312\n",
            "Epoch 8/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 175ms/step - loss: 0.8962\n",
            "Epoch 8: val_loss improved from 1.33121 to 1.31508, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.9150 - val_loss: 1.3151\n",
            "Epoch 9/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 175ms/step - loss: 0.8739\n",
            "Epoch 9: val_loss improved from 1.31508 to 1.29313, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.8582 - val_loss: 1.2931\n",
            "Epoch 10/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.7727\n",
            "Epoch 10: val_loss improved from 1.29313 to 1.26416, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8020 - val_loss: 1.2642\n",
            "Epoch 11/50\n",
            "\u001b[1m5/9\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.7769\n",
            "Epoch 11: val_loss improved from 1.26416 to 1.24846, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7736 - val_loss: 1.2485\n",
            "Epoch 12/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - loss: 0.7157\n",
            "Epoch 12: val_loss improved from 1.24846 to 1.20993, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.7373 - val_loss: 1.2099\n",
            "Epoch 13/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6893\n",
            "Epoch 13: val_loss improved from 1.20993 to 1.16528, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7041 - val_loss: 1.1653\n",
            "Epoch 14/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6846\n",
            "Epoch 14: val_loss improved from 1.16528 to 1.12601, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.6858 - val_loss: 1.1260\n",
            "Epoch 15/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 170ms/step - loss: 0.6414\n",
            "Epoch 15: val_loss improved from 1.12601 to 1.06032, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6625 - val_loss: 1.0603\n",
            "Epoch 16/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - loss: 0.6660\n",
            "Epoch 16: val_loss improved from 1.06032 to 1.02448, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6456 - val_loss: 1.0245\n",
            "Epoch 17/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6510\n",
            "Epoch 17: val_loss improved from 1.02448 to 0.95871, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.6307 - val_loss: 0.9587\n",
            "Epoch 18/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.6244\n",
            "Epoch 18: val_loss improved from 0.95871 to 0.88208, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.6009 - val_loss: 0.8821\n",
            "Epoch 19/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5751\n",
            "Epoch 19: val_loss improved from 0.88208 to 0.83075, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5822 - val_loss: 0.8308\n",
            "Epoch 20/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 166ms/step - loss: 0.5725\n",
            "Epoch 20: val_loss improved from 0.83075 to 0.77361, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5680 - val_loss: 0.7736\n",
            "Epoch 21/50\n",
            "\u001b[1m4/9\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.5486 \n",
            "Epoch 21: val_loss improved from 0.77361 to 0.75821, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.5498 - val_loss: 0.7582\n",
            "Epoch 22/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5230\n",
            "Epoch 22: val_loss improved from 0.75821 to 0.71472, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.5325 - val_loss: 0.7147\n",
            "Epoch 23/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.5228\n",
            "Epoch 23: val_loss improved from 0.71472 to 0.69291, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5207 - val_loss: 0.6929\n",
            "Epoch 24/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.5149\n",
            "Epoch 24: val_loss improved from 0.69291 to 0.68053, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5110 - val_loss: 0.6805\n",
            "Epoch 25/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4915\n",
            "Epoch 25: val_loss improved from 0.68053 to 0.65034, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4981 - val_loss: 0.6503\n",
            "Epoch 26/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4837\n",
            "Epoch 26: val_loss did not improve from 0.65034\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4844 - val_loss: 0.6578\n",
            "Epoch 27/50\n",
            "\u001b[1m5/9\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4779\n",
            "Epoch 27: val_loss improved from 0.65034 to 0.63240, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.4771 - val_loss: 0.6324\n",
            "Epoch 28/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4518\n",
            "Epoch 28: val_loss did not improve from 0.63240\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4642 - val_loss: 0.6450\n",
            "Epoch 29/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.4581\n",
            "Epoch 29: val_loss did not improve from 0.63240\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4575 - val_loss: 0.6386\n",
            "Epoch 30/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4737\n",
            "Epoch 30: val_loss did not improve from 0.63240\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4549 - val_loss: 0.6389\n",
            "Epoch 31/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.4362\n",
            "Epoch 31: val_loss improved from 0.63240 to 0.60809, saving model to ./checkpoint/mlp_var0.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.4439 - val_loss: 0.6081\n",
            "Epoch 32/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.4288\n",
            "Epoch 32: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4366 - val_loss: 0.6443\n",
            "Epoch 33/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.4204\n",
            "Epoch 33: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4301 - val_loss: 0.6236\n",
            "Epoch 34/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.4216\n",
            "Epoch 34: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4266 - val_loss: 0.6471\n",
            "Epoch 35/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.4126\n",
            "Epoch 35: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4142 - val_loss: 0.6215\n",
            "Epoch 36/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.4086\n",
            "Epoch 36: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.4110 - val_loss: 0.6543\n",
            "Epoch 37/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4234\n",
            "Epoch 37: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.4135 - val_loss: 0.6418\n",
            "Epoch 38/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3824\n",
            "Epoch 38: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.3964 - val_loss: 0.6350\n",
            "Epoch 39/50\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.3992 \n",
            "Epoch 39: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3990 - val_loss: 0.6420\n",
            "Epoch 40/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.3819\n",
            "Epoch 40: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3914 - val_loss: 0.6421\n",
            "Epoch 41/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - loss: 0.3874\n",
            "Epoch 41: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3895 - val_loss: 0.6421\n",
            "Epoch 42/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 156ms/step - loss: 0.3809\n",
            "Epoch 42: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3842 - val_loss: 0.6575\n",
            "Epoch 43/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.3674\n",
            "Epoch 43: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3782 - val_loss: 0.6475\n",
            "Epoch 44/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3900\n",
            "Epoch 44: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.3797 - val_loss: 0.6533\n",
            "Epoch 45/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3531\n",
            "Epoch 45: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.3713 - val_loss: 0.6560\n",
            "Epoch 46/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3899\n",
            "Epoch 46: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3755 - val_loss: 0.6580\n",
            "Epoch 47/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.3702\n",
            "Epoch 47: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3676 - val_loss: 0.6467\n",
            "Epoch 48/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.3587\n",
            "Epoch 48: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3667 - val_loss: 0.6685\n",
            "Epoch 49/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.3709\n",
            "Epoch 49: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.3604 - val_loss: 0.6652\n",
            "Epoch 50/50\n",
            "\u001b[1m1/9\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.3737\n",
            "Epoch 50: val_loss did not improve from 0.60809\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3625 - val_loss: 0.6497\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "\n",
        "# Checkpoint directory for saving models\n",
        "checkpoint_dir = \"./checkpoint/\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "num_variables = 1  # We only train one model for ozone forecast\n",
        "\n",
        "mlp_predictions = []\n",
        "training_histories = {}\n",
        "\n",
        "for var_idx in range(num_variables):\n",
        "    print(f\"\\nTraining model for variable {var_idx + 1}/{num_variables}\")\n",
        "\n",
        "    # Flatten multivariate past window: (samples, 336, 2) → (samples, 672)\n",
        "    X_train_single = X_train_vars.reshape(X_train_vars.shape[0], -1)\n",
        "    X_test_single  = X_test_vars.reshape(X_test_vars.shape[0], -1)\n",
        "\n",
        "    # Ozone targets: shape (samples, 96)\n",
        "    y_train_single = y_train_o3.reshape(y_train_o3.shape[0], -1)\n",
        "\n",
        "    # Define model checkpoint file for saving\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"mlp_var{var_idx}.h5\")\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading existing model for variable {var_idx + 1}\")\n",
        "        mlp_model = load_model(checkpoint_path, custom_objects={\"mse\": MeanSquaredError()})\n",
        "    else:\n",
        "        print(f\"Training new model for variable {var_idx + 1}\")\n",
        "\n",
        "        # Build MLP model: input is flattened past (672,), output is 96 future steps\n",
        "        mlp_model = Sequential([\n",
        "            Input(shape=(X_train_single.shape[1],)),\n",
        "            Dense(mlp_hidden_units[0], activation=activation_fn)\n",
        "        ])\n",
        "\n",
        "        # Add additional hidden layers\n",
        "        for units in mlp_hidden_units[1:]:\n",
        "            mlp_model.add(Dense(units, activation=activation_fn))\n",
        "\n",
        "        # Final output layer predicts 96 ozone values\n",
        "        mlp_model.add(Dense(y_train_single.shape[1]))\n",
        "        mlp_model.compile(optimizer=mlp_optim, loss=mlp_loss)\n",
        "\n",
        "        # Save the best model during training\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            checkpoint_path, monitor=\"val_loss\", save_best_only=True, verbose=1\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        training = mlp_model.fit(\n",
        "            X_train_single, y_train_single,\n",
        "            epochs=mlp_epochs, batch_size=mlp_batch_size,\n",
        "            validation_split=0.2, verbose=1,\n",
        "            callbacks=[checkpoint_callback]\n",
        "        )\n",
        "        training_histories[var_idx] = training.history\n",
        "\n",
        "    # Predict future ozone values\n",
        "    y_pred_single = mlp_model.predict(X_test_single.astype(np.float32))\n",
        "    mlp_predictions.append(y_pred_single)\n",
        "\n",
        "# Reshape predictions to (samples, 96, num_variables)\n",
        "mlp_predictions = np.concatenate(mlp_predictions, axis=-1).reshape(y_test_o3.shape[0], prediction_horizon, num_variables)\n",
        "y_pred_single = y_pred_single.reshape(y_pred_single.shape[0], prediction_horizon, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_7EOWfEJzX27",
      "metadata": {
        "id": "_7EOWfEJzX27"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb12fce9",
      "metadata": {
        "id": "cb12fce9"
      },
      "outputs": [],
      "source": [
        "# Reshape ground truth y_test to match the prediction structure: (samples, pred_horizon, num_variables)\n",
        "y_test_reshaped = y_test_o3.reshape(y_test_o3.shape[0], prediction_horizon, num_variables)\n",
        "\n",
        "# Slice out the ozone results (only one page since num_variables=1)\n",
        "y_test_selected = y_test_reshaped[:, :, var_idx]\n",
        "y_pred_selected = mlp_predictions[:, :, var_idx]\n",
        "\n",
        "# Compare predicted vs. true ozone curves for all test samples\n",
        "evaluate_model(y_test_selected, y_pred_selected)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e6f010",
      "metadata": {
        "id": "15e6f010"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Load ozone model\n",
        "mlp_model = load_model(\n",
        "    os.path.join(checkpoint_dir, \"mlp_var0.h5\"),\n",
        "    custom_objects={\"mse\": MeanSquaredError()}\n",
        ")\n",
        "\n",
        "# Just take first test sample (no station filtering needed if single-station)\n",
        "first_idx = 0\n",
        "\n",
        "# Extract context inputs (temp, o3) and future ozone\n",
        "context_temp = X_test_vars[first_idx, :, 0] * scaler_stats[\"temp\"]['std'] + scaler_stats[\"temp\"]['mean']\n",
        "context_o3   = X_test_vars[first_idx, :, 1] * scaler_stats[\"o3\"]['std'] + scaler_stats[\"o3\"]['mean']\n",
        "\n",
        "actual_future = y_test_o3[first_idx, :] * scaler_stats[\"o3\"]['std'] + scaler_stats[\"o3\"]['mean']\n",
        "\n",
        "# Flatten inputs for model\n",
        "X_flat = X_test_vars[first_idx].reshape(1, -1).astype(np.float32)\n",
        "\n",
        "# Predict future ozone\n",
        "pred_future = mlp_model.predict(X_flat).flatten()\n",
        "predicted_future = pred_future * scaler_stats[\"o3\"]['std'] + scaler_stats[\"o3\"]['mean']\n",
        "\n",
        "# Plot\n",
        "plt.plot(range(context_window), context_temp, label=\"Context Temp\", color=\"lightblue\")\n",
        "plt.plot(range(context_window), context_o3, label=\"Context O3\", color=\"blue\")\n",
        "plt.plot(range(context_window, context_window + prediction_horizon), actual_future, label=\"Actual Future O3\", color=\"green\", marker=\"o\")\n",
        "plt.plot(range(context_window, context_window + prediction_horizon), predicted_future, label=\"MLP Prediction O3\", linestyle=\"--\", color=\"orange\", marker=\"x\")\n",
        "\n",
        "plt.title(\"MLP Forecast for Ozone (First Test Sample)\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save forecast\n",
        "save_forecast(\n",
        "    station=\"DENW094\",\n",
        "    model=\"MLP\",\n",
        "    episode_id=1,\n",
        "    context_vals=context_o3,\n",
        "    future_true=actual_future,\n",
        "    future_pred=predicted_future,\n",
        "    folder=f\"{BASEPATH}/hw_results\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e68c7a",
      "metadata": {
        "id": "64e68c7a"
      },
      "source": [
        "# Observations for Task 2\n",
        "\n",
        "- I was able to generate valid sequences for multivariate forecasting after fixing the dataset.\n",
        "- The issue was duplicate timestamps, which initially caused zero valid windows.\n",
        "- After dropping duplicates per hour, the create_sequences function worked, though I'm not sure this is the best solution.\n",
        "- Later during model training, validation losses were not improving over epochs with the default hyperparameters.\n",
        "- I increased the batch size as I have seen in the literature that a batch size of 10% of the training dataset size is usually used.\n",
        "- I also decreased the hidden units, as I thought the model was overfitting since training losses were improving without improvement (and actually worsening) validation loss. This change in hyperparameters improved validation loss from approx. 0.9 to 0.7.\n",
        "-  However the prediction is still quite poor. I believe dropping duplicates might have caused a shift in the distribution for which we need to normalize again. Therefore we should drop duplicates before any preprocessing steps (in my opinion).\n",
        "- I also think the amount of data is actually too less to expect good learning from the model (since we could not load all the data into the Colab environment)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcee8610",
      "metadata": {
        "id": "dcee8610"
      },
      "source": [
        "# Problems with Data Download\n",
        "I could not redo the data download and normalisation after dropping duplicates process because the data download failed when I tried.\n",
        "\n",
        "The results I got in this notebook are from the normalised data given in Notebook 1 in the time series folder in the repo."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Unl5lVa-n65g"
      },
      "id": "Unl5lVa-n65g",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}