{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babypanda212/mless/blob/main/remote_sensing/hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "868c2472",
      "metadata": {
        "id": "868c2472"
      },
      "source": [
        "# Homework 1: CNN vs ResNet on SAT-6 Dataset\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. **Imports**\n",
        "2. **SkipLayer**\n",
        "    - **Implementation**  \n",
        "    - **Test**  \n",
        "3. Dataset Preparation\n",
        "    - **Download SAT-6 Dataset**\n",
        "    - **Dataset Exploration**\n",
        "    - **Dataset Preprocessing**\n",
        "4. Model Architectures\n",
        "    - **Vanilla CNN**\n",
        "    - **Torchvision ResNet18 (Modified)**\n",
        "5. Training Pipelines\n",
        "    - **Training the Vanilla CNN**\n",
        "    - **Training ResNet18**\n",
        "6. Evaluation\n",
        "    - **Define Metrics (Kappa, ROC)**\n",
        "    - **Evaluate Vanilla CNN**\n",
        "    - **Evaluate ResNet18**\n",
        "7. Observations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f5a4f0e",
      "metadata": {
        "id": "6f5a4f0e"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "12a785aa-ce97-4c35-ab1f-1675d4ad4535",
      "metadata": {
        "id": "12a785aa-ce97-4c35-ab1f-1675d4ad4535"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce05edb",
      "metadata": {
        "id": "6ce05edb"
      },
      "source": [
        "# SkipLayer\n",
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "10450c1b",
      "metadata": {
        "id": "10450c1b"
      },
      "outputs": [],
      "source": [
        "## Write a torch module for one skip layer as shown in the figure below\n",
        "\n",
        "## A layer in the figure represent a sequential layer of conv layer -> batchnorm -> Activation function(ReLu)\n",
        "\n",
        "## We expect by now, you have learnt about different arguments of torch.nn.Conv2D module like in_channels, out_channels, kernel_size, stride, and padding\n",
        "\n",
        "##import modules whichever required\n",
        "\n",
        "class SkipLayer(torch.nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    # Define the layers as shown in the figure\n",
        "\n",
        "    # Define conv layer\n",
        "    self.conv = torch.nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
        "    # Define batchnorm layer\n",
        "    self.bn = torch.nn.BatchNorm2d(num_features=4)\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Implement the forward pass as shown in the figure\n",
        "    # Apply conv layer, batchnorm, and relu activation\n",
        "    out = self.conv(x)\n",
        "    out = self.bn(out)\n",
        "    out = F.relu(out)\n",
        "    # Add the input x to the output - this is the skip connection\n",
        "    out = out + x\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2ebc94d",
      "metadata": {
        "id": "a2ebc94d"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7b4dce0f",
      "metadata": {
        "id": "7b4dce0f",
        "outputId": "6db14cdd-f1d4-4d16-b5cb-4f580da25e42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 4, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "## Test your module\n",
        "\n",
        "# 16-batchsize\n",
        "# 4-channels\n",
        "# 28 \\times 28 - height \\times width\n",
        "\n",
        "random_sample = torch.randn((16,4,28,28))\n",
        "skip_layer = SkipLayer()\n",
        "print(skip_layer(random_sample).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faa3e7f1",
      "metadata": {
        "id": "faa3e7f1"
      },
      "source": [
        "# Dataset Preparation\n",
        "## Download SAT-6 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438895d8-d0fc-4317-bb96-f445104fbb9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "438895d8-d0fc-4317-bb96-f445104fbb9e",
        "outputId": "2e147e70-d2c4-448b-f9e2-78a7550f5ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-02 09:41:56--  https://b2share.eudat.eu/api/files/a697daf7-7570-44ff-854c-0fab43f2b52c/X_test_sat6.csv\n",
            "Resolving b2share.eudat.eu (b2share.eudat.eu)... 86.50.166.97\n",
            "Connecting to b2share.eudat.eu (b2share.eudat.eu)|86.50.166.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 905628829 (864M) [text/plain]\n",
            "Saving to: ‘X_test_sat6.csv’\n",
            "\n",
            "X_test_sat6.csv      81%[===============>    ] 701.07M  16.9MB/s    eta 10s    "
          ]
        }
      ],
      "source": [
        "!wget https://b2share.eudat.eu/api/files/a697daf7-7570-44ff-854c-0fab43f2b52c/X_test_sat6.csv\n",
        "!wget https://b2share.eudat.eu/api/files/a697daf7-7570-44ff-854c-0fab43f2b52c/y_test_sat6.csv\n",
        "!wget https://b2share.eudat.eu/api/files/a697daf7-7570-44ff-854c-0fab43f2b52c/sat6annotations.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b13df99",
      "metadata": {
        "id": "4b13df99"
      },
      "source": [
        "## Dataset Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930dc41c-e96c-4c00-8716-cea2ac8c4fbc",
      "metadata": {
        "id": "930dc41c-e96c-4c00-8716-cea2ac8c4fbc"
      },
      "outputs": [],
      "source": [
        "# load the csv file with the image data. Each row belongs to one sample. Each sample has 3136 columns i.e. 4 channels (R,G,B, and NIR) and 28 \\times 28 spatial size\n",
        "landcover_df = pd.read_csv(\"./X_test_sat6.csv\",header=None)\n",
        "landcover_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z_DFKShn707E",
      "metadata": {
        "id": "Z_DFKShn707E"
      },
      "source": [
        "### Read annotation labels from different csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077fafc9-9ddd-4ce2-8895-13583da7840d",
      "metadata": {
        "id": "077fafc9-9ddd-4ce2-8895-13583da7840d"
      },
      "outputs": [],
      "source": [
        "# Read the annotation file to get the annotation i.e. classname corresponding to the labels\n",
        "annotations = pd.read_csv(\"./sat6annotations.csv\", header=None)\n",
        "print(annotations)\n",
        "labels = annotations[0].values\n",
        "print(f'class labels: {labels}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ev1wNGX4HP4r",
      "metadata": {
        "id": "ev1wNGX4HP4r"
      },
      "source": [
        "**Reminder :**  The data structure that you see above is called _one-hot encoding_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oqw6u5AJnJMH",
      "metadata": {
        "id": "Oqw6u5AJnJMH"
      },
      "outputs": [],
      "source": [
        "# load the csv file with the labels of all samples\n",
        "labels_df = pd.read_csv(\"./y_test_sat6.csv\",header=None)\n",
        "print(labels_df)\n",
        "# get the names of the class labels (here, only \"0\" to \"5\")\n",
        "# These correspond to the labels printed above\n",
        "column_names = labels_df.columns.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7f8852-000c-473e-bf2a-aabd2de04775",
      "metadata": {
        "id": "bc7f8852-000c-473e-bf2a-aabd2de04775"
      },
      "source": [
        "### Plot random samples for illustration\n",
        "Plot few samples to see visually the data and gain more insights on how to reshape the 1d data to a 4 channel image (RGB & NIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CS82-0LrtN3U",
      "metadata": {
        "id": "CS82-0LrtN3U"
      },
      "outputs": [],
      "source": [
        "num_classes = len(column_names)\n",
        "num_samples = 8  # number of rows to plot\n",
        "sample_idx = []\n",
        "for column in column_names:\n",
        "    # find all indices of a given class\n",
        "    class_idx = labels_df[column] == 1\n",
        "    # randomly select num_samples from this index list - make sure to avoid duplicates\n",
        "    sample_idx.append(np.random.choice(np.where(class_idx.values)[0], size=num_samples, replace=False).tolist())\n",
        "\n",
        "fig, ax = plt.subplots(num_samples, num_classes, figsize=(20,20))\n",
        "for i in range(num_samples):\n",
        "    for j in range(num_classes):\n",
        "        data_index = sample_idx[j][i]\n",
        "        ax[i,j].set_title(f\"{labels[j]}, {data_index}\")\n",
        "        ax[i,j].axis('off')\n",
        "        data = np.reshape(landcover_df.iloc[data_index].values,(-1,28,28,4))\n",
        "        ax[i,j].imshow(data[0,...,:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e7987d",
      "metadata": {
        "id": "f3e7987d"
      },
      "source": [
        "## Dataset Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83fda11f-d647-4536-a148-0ec3e9a25410",
      "metadata": {
        "id": "83fda11f-d647-4536-a148-0ec3e9a25410"
      },
      "source": [
        "### Create the training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rV1V5R63p0Dv",
      "metadata": {
        "id": "rV1V5R63p0Dv"
      },
      "outputs": [],
      "source": [
        "num_train = 1000 #replace with -1 if all samples need to be used for training\n",
        "num_test = 100\n",
        "train_idx = []\n",
        "test_idx = []\n",
        "for column in column_names:\n",
        "    # find all indices of a given class\n",
        "    class_idx = labels_df[column] == 1\n",
        "    # randomly select num_train and num_test values from this index list - make sure to avoid duplicates\n",
        "    valid_indices = np.where(class_idx.values)[0]\n",
        "    random_indices = np.random.permutation(valid_indices)\n",
        "    test_idx.extend(random_indices[:num_test])\n",
        "    train_idx.extend(random_indices[num_test:num_test+num_train if num_train != -1 else num_train])\n",
        "print(f'number of train indices: {len(train_idx)}, number of test indices: {len(test_idx)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd248f3-1f78-4043-ba4e-c64819c5aa8f",
      "metadata": {
        "id": "edd248f3-1f78-4043-ba4e-c64819c5aa8f"
      },
      "outputs": [],
      "source": [
        "# Extract images and labels corresponding to the selected indices\n",
        "train_X = landcover_df.iloc[train_idx]\n",
        "train_y = labels_df.iloc[train_idx]\n",
        "test_X = landcover_df.iloc[test_idx]\n",
        "test_y = labels_df.iloc[test_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3U3P88T53hKE",
      "metadata": {
        "id": "3U3P88T53hKE"
      },
      "source": [
        "### Calculate Normalization Parameter using Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UVJ5gu7e4Lt-",
      "metadata": {
        "id": "UVJ5gu7e4Lt-"
      },
      "outputs": [],
      "source": [
        "train_X_reshape = train_X.values.reshape((-1,28,28,4))\n",
        "mean_per_channels = np.mean(train_X_reshape,axis=(0,1,2)).astype(np.float32)\n",
        "std_per_channels = np.std(train_X_reshape,axis=(0,1,2)).astype(np.float32)\n",
        "mean_per_channels, std_per_channels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd0d08c-1896-421d-bec2-aba606be3a04",
      "metadata": {
        "id": "ddd0d08c-1896-421d-bec2-aba606be3a04"
      },
      "source": [
        "### Define torch dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47dd4fee-8501-4225-8922-2b347ab94993",
      "metadata": {
        "id": "47dd4fee-8501-4225-8922-2b347ab94993"
      },
      "outputs": [],
      "source": [
        "class SAT6Dataset(Dataset):\n",
        "  def __init__(self,data_df,label_df, normalization_parameter):\n",
        "    super(SAT6Dataset,self).__init__()\n",
        "    self.data_df = data_df\n",
        "    self.label_df = label_df\n",
        "    self.normalization = normalization_parameter\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data_df)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    data = self.data_df.iloc[idx].values.reshape((28,28,4)).astype(np.float32)\n",
        "    data = (data-self.normalization[0])/self.normalization[1]\n",
        "    #data = data/255.0\n",
        "    data = np.transpose(data,axes=(2,0,1))\n",
        "    label = self.label_df.iloc[idx].values\n",
        "    label_args = np.where(label==1)[0]\n",
        "\n",
        "    return (data,label_args[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pgkAjW_E0_er",
      "metadata": {
        "id": "pgkAjW_E0_er"
      },
      "outputs": [],
      "source": [
        "# define two dataset one for train and other for test\n",
        "sat6_train_dataset = SAT6Dataset(train_X,train_y,(mean_per_channels,std_per_channels))\n",
        "sat6_test_dataset = SAT6Dataset(test_X,test_y,(mean_per_channels,std_per_channels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9370509",
      "metadata": {
        "id": "a9370509"
      },
      "source": [
        "# Model Architectures\n",
        "## Vanilla CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kiJ2nVW51JKV",
      "metadata": {
        "id": "kiJ2nVW51JKV"
      },
      "outputs": [],
      "source": [
        "# previously defined vanilla CNN\n",
        "class CNN(torch.nn.Module):\n",
        "  # Constructor for the CNN\n",
        "  def __init__(self,\n",
        "               input_shape,\n",
        "               num_conv_layers,\n",
        "               channels_each_layer,\n",
        "               kernel_size,\n",
        "               mlp_dimension,\n",
        "               activation_func,\n",
        "               pooling,\n",
        "               input_channels,\n",
        "               num_classes,\n",
        "               norm=\"batch_norm\"):\n",
        "    super(CNN,self).__init__()\n",
        "    self.num_conv_layers = num_conv_layers\n",
        "    self.channels_each_layer = channels_each_layer\n",
        "    # check if the number of convolutional layers matches the length of channels_each_layer - to ensure that each layer has a specified number of channels\n",
        "    assert num_conv_layers == len(channels_each_layer),\"Number of conv layers does not match with length of channels given\"\n",
        "    # if kernel size is a single integer, then use the same kernel size for all layers else use the kernel size given for each layer\n",
        "    self.kernel_size = ([kernel_size] * num_conv_layers\n",
        "                        if type(kernel_size) == int\n",
        "                        else kernel_size)\n",
        "    self.mlp_dimension = mlp_dimension\n",
        "    if activation_func == 'relu':\n",
        "      self.activation_func = torch.nn.ReLU()\n",
        "    elif activation_func == 'tanh':\n",
        "      self.activation_func = torch.nn.Tanh()\n",
        "    else:\n",
        "      assert False, \"Currently only relu and tanh are provided in this code\"\n",
        "\n",
        "    if pooling == \"max\":\n",
        "      self.pooling = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    else:\n",
        "      self.pooling = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.input_channels = input_channels\n",
        "    if norm==\"batch_norm\":\n",
        "      self.batch_norms = [torch.nn.BatchNorm2d(channels_each_layer[i])\n",
        "                          for i in range(len(self.kernel_size))] # kernel_size is a list from which we can get the number of layers because the list defines a kernel size for each layer\n",
        "    elif norm == \"identity\":\n",
        "      self.batch_norms = [torch.nn.Identity() for i in range(len(self.kernel_size))]\n",
        "    else:\n",
        "      assert False,\"only batch norm is provided in this code\"\n",
        "\n",
        "    self.conv_modules = torch.nn.ModuleList()\n",
        "\n",
        "    for i in range(num_conv_layers):\n",
        "      self.conv_modules.append(\n",
        "          torch.nn.Conv2d(input_channels if i==0 else channels_each_layer[i-1],\n",
        "                          channels_each_layer[i],\n",
        "                          self.kernel_size[i],\n",
        "                          stride=1,\n",
        "                          padding='same',\n",
        "                          bias=True))\n",
        "      self.conv_modules.append(self.batch_norms[i])\n",
        "      self.conv_modules.append(self.activation_func)\n",
        "      self.conv_modules.append(self.pooling)\n",
        "    dim_after_final_conv_layer = (input_shape[0]//2**num_conv_layers)*(\n",
        "        input_shape[1]//2**num_conv_layers)*(\n",
        "            channels_each_layer[-1])\n",
        "    self.mlp_layer_modules = torch.nn.ModuleList()\n",
        "    self.mlp_dimension = ([mlp_dimension]\n",
        "                          if type(mlp_dimension) == int\n",
        "                          else mlp_dimension)\n",
        "\n",
        "    for i in range(len(self.mlp_dimension)):\n",
        "      self.mlp_layer_modules.append(\n",
        "          torch.nn.Linear((dim_after_final_conv_layer\n",
        "                           if i==0\n",
        "                           else self.mlp_dimension[i-1]),\n",
        "                          self.mlp_dimension[i]))\n",
        "      # Incase one want to try batch norm uncomment below line\n",
        "      #self.mlp_layer_modules.append(torch.nn.BatchNorm1d(mlp_dimension[i]))\n",
        "      self.mlp_layer_modules.append(self.activation_func)\n",
        "    self.mlp_layer_modules.append(\n",
        "        torch.nn.Linear(self.mlp_dimension[-1],num_classes))\n",
        "\n",
        "  def forward(self,x):\n",
        "    for module in self.conv_modules:\n",
        "      x = module(x)\n",
        "    x = x.flatten(1)\n",
        "    for module in self.mlp_layer_modules:\n",
        "      x = module(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YqR6eVM8_HEQ",
      "metadata": {
        "id": "YqR6eVM8_HEQ"
      },
      "source": [
        "Define the device (**GPU** if you use GPU as your runtime) else it will run on **CPU**)\n",
        "Instatiation of the CNN model is also define in this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eYk86tpKzJHZ",
      "metadata": {
        "id": "eYk86tpKzJHZ"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Initialize the CNN model with the specified parameters ie.\n",
        "# input shape, number of convolutional layers, channels in each layer, kernel size for each layer,\n",
        "# activation function, pooling function, input channels, number of classes, and normalization function\n",
        "cnn = CNN((28,28),3,[32,64,128],[5,3,3],[32],\"relu\",\"max\",4,6,norm=\"identity\").to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4kVZOLcqZUcb",
      "metadata": {
        "id": "4kVZOLcqZUcb"
      },
      "outputs": [],
      "source": [
        "# Overview of the model\n",
        "print(cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae2790d",
      "metadata": {
        "id": "9ae2790d"
      },
      "source": [
        "## Torchvision Resnet18 (Modified)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# implement resnet18, modified to accept 4 channels as input\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# load resnet18\n",
        "from"
      ],
      "metadata": {
        "id": "FslELgnHBPHY"
      },
      "id": "FslELgnHBPHY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0c51685b",
      "metadata": {
        "id": "0c51685b"
      },
      "source": [
        "# Training Pipelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9c3a4bf",
      "metadata": {
        "id": "b9c3a4bf"
      },
      "source": [
        "### Set Optimiser and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dV9My2ReMkBT",
      "metadata": {
        "id": "dV9My2ReMkBT"
      },
      "outputs": [],
      "source": [
        "optim = \"adam\"\n",
        "optimizer = torch.optim.Adam(cnn.parameters(),lr=0.001) if optim == \"adam\" else torch.optim.SGD(cnn.parameters(),lr=0.1)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.1)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a67553d8-7b9f-4e30-972d-1cdcd5b76db0",
      "metadata": {
        "id": "a67553d8-7b9f-4e30-972d-1cdcd5b76db0"
      },
      "source": [
        "### Construct Dataset Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9H0LGILS44o",
      "metadata": {
        "id": "c9H0LGILS44o"
      },
      "outputs": [],
      "source": [
        "# construct dataloader with batch size 256 and shuffle enabled for training data\n",
        "train_dataloader = DataLoader(sat6_train_dataset,batch_size=256,shuffle=True,drop_last=True)\n",
        "# construct dataloader with full test dataset and shuffle disabled for test data\n",
        "test_dataloader = DataLoader(sat6_test_dataset,len(sat6_test_dataset), shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GsXleVOvUYce",
      "metadata": {
        "id": "GsXleVOvUYce"
      },
      "source": [
        "## Training the Vanilla CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PeQY30XxTrpL",
      "metadata": {
        "id": "PeQY30XxTrpL"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "training_loss_per_epoch = []\n",
        "validation_loss_per_epoch = []\n",
        "acc_per_epoch = []\n",
        "\n",
        "with tqdm(total=num_epochs) as pbar:\n",
        "  for i in range(num_epochs):\n",
        "    cnn.train()\n",
        "    for train_data_idx, train_data in enumerate(train_dataloader):\n",
        "      optimizer.zero_grad()\n",
        "      data, labels = train_data\n",
        "      data = data.to(device)\n",
        "      labels = labels.to(device)\n",
        "      preds = cnn(data)\n",
        "      train_loss = loss_fn(preds,labels)\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "    # uncomment incase scheduler is defined above\n",
        "    #scheduler.step()\n",
        "    training_loss_per_epoch.append(train_loss.item())\n",
        "\n",
        "    cnn.eval()\n",
        "    pred_labels = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "      for test_data_idx, test_data in enumerate(test_dataloader):\n",
        "        data, labels = test_data\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        preds = cnn(data)\n",
        "        val_loss = loss_fn(preds,labels)\n",
        "        pred_labels.extend(torch.argmax(preds,dim=1).to('cpu'))\n",
        "        true_labels.extend(torch.squeeze(labels).to('cpu'))\n",
        "    validation_loss_per_epoch.append(val_loss.item())\n",
        "    acc = accuracy_score(true_labels,pred_labels)\n",
        "    acc_per_epoch.append(acc)\n",
        "    pbar.set_description(f\"Epoch {i+1}/{num_epochs}: training_loss : {train_loss}, validation_loss : {val_loss} and accuracy : {acc} \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "682d2917",
      "metadata": {
        "id": "682d2917"
      },
      "source": [
        "## Training ResNet18"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4527be7",
      "metadata": {
        "id": "e4527be7"
      },
      "source": [
        "# Evaluation\n",
        "## Define Metrics (Kappa, ROC)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a8cfef",
      "metadata": {
        "id": "04a8cfef"
      },
      "source": [
        "## Evaluate Vanilla CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b91d31c",
      "metadata": {
        "id": "5b91d31c"
      },
      "source": [
        "## Evaluate ResNet18"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086b637f",
      "metadata": {
        "id": "086b637f"
      },
      "source": [
        "# Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7e3a76",
      "metadata": {
        "id": "4f7e3a76"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}